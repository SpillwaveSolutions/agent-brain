---
phase: 06-postgresql-backend
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - agent-brain-server/agent_brain_server/storage/postgres/__init__.py
  - agent-brain-server/agent_brain_server/storage/postgres/config.py
  - agent-brain-server/agent_brain_server/storage/postgres/connection.py
  - agent-brain-server/agent_brain_server/storage/postgres/schema.py
  - agent-brain-server/templates/docker-compose.postgres.yml
  - agent-brain-plugin/templates/docker-compose.postgres.yml
autonomous: true
must_haves:
  truths:
    - "PostgresConfig validates all connection parameters with sensible defaults"
    - "DATABASE_URL env var overrides YAML connection fields"
    - "Connection pool creates async engine with configurable pool_size and max_overflow"
    - "Connection pool reports status metrics (size, checked_in, checked_out, overflow)"
    - "Retry with exponential backoff handles Docker containers still initializing"
    - "Schema SQL creates documents table with dynamic vector dimension"
    - "Schema SQL creates HNSW index, GIN index for tsvector, and GIN index for metadata"
    - "Embedding dimension mismatch detected on startup raises StorageError"
    - "Docker Compose template starts pgvector/pgvector:pg16 with named volume"
  artifacts:
    - path: "agent-brain-server/agent_brain_server/storage/postgres/config.py"
      provides: "PostgresConfig Pydantic model with connection URL builder"
      exports: ["PostgresConfig"]
    - path: "agent-brain-server/agent_brain_server/storage/postgres/connection.py"
      provides: "Async connection pool manager with retry and health metrics"
      exports: ["PostgresConnectionManager"]
    - path: "agent-brain-server/agent_brain_server/storage/postgres/schema.py"
      provides: "SQL schema strings and auto-creation with dimension validation"
      exports: ["PostgresSchemaManager"]
    - path: "agent-brain-server/templates/docker-compose.postgres.yml"
      provides: "Docker Compose template for local PostgreSQL + pgvector"
    - path: "agent-brain-plugin/templates/docker-compose.postgres.yml"
      provides: "Docker Compose template copy for plugin deployment"
  key_links:
    - from: "connection.py"
      to: "config.py"
      via: "PostgresConfig.get_connection_url()"
      pattern: "config\\.get_connection_url"
    - from: "schema.py"
      to: "connection.py"
      via: "PostgresConnectionManager.engine for SQL execution"
      pattern: "connection_manager\\.engine"
---

<objective>
Create the PostgreSQL backend foundation: configuration model, async connection pool manager with retry logic, SQL schema manager with dynamic vector dimensions, and Docker Compose template.

Purpose: These are the building blocks that Plan 02 (vector/keyword operations) and Plan 03 (factory integration) depend on. No existing code is modified -- all new files in a new `storage/postgres/` package.

Output: 4 Python modules (config, connection, schema, __init__) + 2 Docker Compose templates (server + plugin)
</objective>

<execution_context>
@/Users/richardhightower/.claude/get-shit-done/workflows/execute-plan.md
@/Users/richardhightower/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-postgresql-backend/06-CONTEXT.md
@.planning/phases/06-postgresql-backend/06-RESEARCH.md
@.planning/phases/05-storage-abstraction/05-01-SUMMARY.md
@agent-brain-server/agent_brain_server/storage/protocol.py
@agent-brain-server/agent_brain_server/config/provider_config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: PostgresConfig Pydantic model and package init</name>
  <files>
    agent-brain-server/agent_brain_server/storage/postgres/__init__.py
    agent-brain-server/agent_brain_server/storage/postgres/config.py
  </files>
  <action>
Create `storage/postgres/` package directory.

**config.py** -- Create PostgresConfig Pydantic BaseModel with these fields:
- `host: str = "localhost"` -- PostgreSQL host
- `port: int = 5432` -- PostgreSQL port
- `database: str = "agent_brain"` -- Database name
- `user: str = "agent_brain"` -- Database user
- `password: str = ""` -- Database password (empty for local dev)
- `pool_size: int = 10` -- Connection pool size (per user decision: sensible defaults 10)
- `pool_max_overflow: int = 10` -- Max overflow (total max = pool_size + overflow = 20, per user decision)
- `language: str = "english"` -- tsvector language for full-text search
- `hnsw_m: int = 16` -- HNSW index m parameter
- `hnsw_ef_construction: int = 64` -- HNSW ef_construction parameter
- `debug: bool = False` -- Enable SQL echo logging

Add `@field_validator("language")` that validates against supported PostgreSQL languages: english, spanish, french, german, italian, portuguese, russian, simple. Case-insensitive (normalize to lowercase).

Add `@field_validator("port")` that validates port range (1-65535).

Add `get_connection_url() -> str` method that builds `postgresql+asyncpg://user:password@host:port/database`. URL-encode password using `urllib.parse.quote_plus` to handle special characters.

Add `@classmethod from_database_url(cls, url: str) -> PostgresConfig` that parses a DATABASE_URL string into config fields. Use `urllib.parse.urlparse` to extract components. This handles the user decision: "DATABASE_URL env var as override."

**__init__.py** -- Export PostgresConfig. (PostgresBackend will be added in Plan 02.)

All type hints required (mypy strict). Google-style docstrings. Black-formatted (88 char line length).
  </action>
  <verify>
```bash
cd agent-brain-server && poetry run python -c "from agent_brain_server.storage.postgres.config import PostgresConfig; c = PostgresConfig(); print(c.get_connection_url()); print('OK')"
```
Verify output contains `postgresql+asyncpg://agent_brain@localhost:5432/agent_brain` and `OK`.

```bash
cd agent-brain-server && poetry run mypy agent_brain_server/storage/postgres/config.py --strict
```
  </verify>
  <done>PostgresConfig model validates all fields, builds connection URL, parses DATABASE_URL, passes mypy strict.</done>
</task>

<task type="auto">
  <name>Task 2: Connection pool manager with retry logic</name>
  <files>agent-brain-server/agent_brain_server/storage/postgres/connection.py</files>
  <action>
Create `PostgresConnectionManager` class in connection.py:

**Constructor:** Accept `PostgresConfig` instance. Store as `self.config`. Initialize `self._engine: AsyncEngine | None = None`.

**`async initialize() -> None`:** Create SQLAlchemy async engine via `create_async_engine()` with:
- `connection_url` from `self.config.get_connection_url()`
- `echo=self.config.debug`
- `pool_size=self.config.pool_size`
- `max_overflow=self.config.pool_max_overflow`
- `pool_pre_ping=True` (validate connections before use)
- `pool_recycle=3600` (recycle after 1 hour)

Log pool configuration on successful creation.

**`async initialize_with_retry(max_attempts: int = 5, initial_delay: float = 1.0, backoff_factor: float = 2.0) -> None`:** Retry loop calling `initialize()`. On failure: log warning, sleep with exponential backoff (1s, 2s, 4s, 8s, 16s). After all attempts fail, raise `StorageError` with message: "PostgreSQL connection failed after {N} attempts. Ensure PostgreSQL is running and accessible at {host}:{port}". Per user decision: "retry with exponential backoff (3-5 attempts) before failing."

**`async close() -> None`:** Dispose engine if initialized. Log closure. Idempotent (safe to call multiple times).

**`engine` property:** Return `self._engine`, raise `RuntimeError("Connection manager not initialized. Call initialize() first.")` if None.

**`async get_pool_status() -> dict[str, Any]`:** Return pool metrics dict for health endpoint (per user decision: "pool metrics (active, idle, size)"):
- `pool_size` from `pool.size()`
- `checked_in` from `pool.checkedin()`
- `checked_out` from `pool.checkedout()`
- `overflow` from `pool.overflow()`
- `total` = size + overflow
If engine not initialized, return `{"status": "not_initialized"}`.

Import `create_async_engine` and `AsyncEngine` from `sqlalchemy.ext.asyncio`. Import `StorageError` from `..protocol`. Import `asyncio` for sleep. Import `logging` for structured logging.

Note: asyncpg and sqlalchemy are optional dependencies. Use conditional import pattern -- these modules will only be imported when backend="postgres" is selected. The actual packages will be added as Poetry extras in Plan 03.
  </action>
  <verify>
```bash
cd agent-brain-server && poetry run mypy agent_brain_server/storage/postgres/connection.py --strict --ignore-missing-imports
```
Verify no type errors (ignore-missing-imports because sqlalchemy/asyncpg not installed yet as extras).
  </verify>
  <done>PostgresConnectionManager creates async engine with configurable pool, retries with exponential backoff, reports pool metrics, and cleanly shuts down.</done>
</task>

<task type="auto">
  <name>Task 3: Schema manager and Docker Compose template</name>
  <files>
    agent-brain-server/agent_brain_server/storage/postgres/schema.py
    agent-brain-server/templates/docker-compose.postgres.yml
    agent-brain-plugin/templates/docker-compose.postgres.yml
  </files>
  <action>
**schema.py** -- Create `PostgresSchemaManager` class:

**Constructor:** Accept `connection_manager: PostgresConnectionManager` and `embedding_dimensions: int`. Store both. Also accept optional `config: PostgresConfig | None = None` for HNSW params and language.

**`async create_schema() -> None`:** Execute SQL statements using `connection_manager.engine`. Use `async with engine.begin() as conn:` for transaction. Execute these SQL statements via `conn.execute(text(...))`:

1. `CREATE EXTENSION IF NOT EXISTS vector;`
2. Create `documents` table:
   - `chunk_id TEXT PRIMARY KEY`
   - `document_text TEXT NOT NULL`
   - `metadata JSONB NOT NULL DEFAULT '{}'`
   - `embedding vector({dimensions})` -- dynamic from constructor
   - `tsv tsvector` -- full-text search column
   - `created_at TIMESTAMPTZ DEFAULT NOW()`
   - `updated_at TIMESTAMPTZ DEFAULT NOW()`
3. Create HNSW index: `CREATE INDEX IF NOT EXISTS documents_embedding_idx ON documents USING hnsw (embedding vector_cosine_ops) WITH (m = {m}, ef_construction = {ef_construction});` -- Use config.hnsw_m and config.hnsw_ef_construction (defaults 16, 64).
4. Create GIN index for tsvector: `CREATE INDEX IF NOT EXISTS documents_tsv_idx ON documents USING gin(tsv);`
5. Create GIN index for metadata: `CREATE INDEX IF NOT EXISTS documents_metadata_idx ON documents USING gin(metadata);`
6. Create `embedding_metadata` table:
   - `id INTEGER PRIMARY KEY DEFAULT 1`
   - `provider TEXT NOT NULL`
   - `model TEXT NOT NULL`
   - `dimensions INTEGER NOT NULL`
   - `created_at TIMESTAMPTZ DEFAULT NOW()`
   - `CONSTRAINT single_row CHECK (id = 1)`

Use `sqlalchemy.text()` for raw SQL. Per user decision: "Embedded SQL in Python code -- schema as SQL strings."

**`async validate_dimensions() -> None`:** Query `embedding_metadata` table for stored dimensions. If stored dimensions exist and differ from `self.embedding_dimensions`, raise `StorageError` with message: "Embedding dimension mismatch: stored={stored}, current={current}. Cannot use index created with different dimensions. Run 'agent-brain reset --yes' to recreate index." Per user decision: "Validated on subsequent startups (mismatch = fail fast)."

**`async store_embedding_metadata(provider: str, model: str, dimensions: int) -> None`:** Upsert into `embedding_metadata` table (INSERT ... ON CONFLICT (id) DO UPDATE).

**`async get_embedding_metadata() -> dict[str, Any] | None`:** Query `embedding_metadata` for provider, model, dimensions. Return dict or None if no row exists.

**Docker Compose templates** -- Create identical files in both locations (per user decision: "Docker Compose template lives in both server package and plugin package"):

```yaml
# Docker Compose for Agent Brain PostgreSQL backend
# Usage: docker compose -f docker-compose.postgres.yml up -d
services:
  postgres:
    image: pgvector/pgvector:pg16
    container_name: agent-brain-postgres
    environment:
      POSTGRES_USER: agent_brain
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-agent_brain_dev}
      POSTGRES_DB: agent_brain
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    volumes:
      - agent-brain-postgres-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U agent_brain -d agent_brain"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped

volumes:
  agent-brain-postgres-data:
    name: agent-brain-postgres-data
    labels:
      project: "agent-brain"
      description: "Agent Brain PostgreSQL data with pgvector indexes"
```

Per user decisions: PostgreSQL 16 + pgvector 0.7+, minimal compose (just PostgreSQL + pgvector container), named Docker volume, dynamic port via env var.

Create `agent-brain-server/templates/` directory if it does not exist. Create `agent-brain-plugin/templates/` directory if it does not exist.
  </action>
  <verify>
```bash
cd agent-brain-server && poetry run mypy agent_brain_server/storage/postgres/schema.py --strict --ignore-missing-imports
```

Verify Docker Compose template is valid YAML:
```bash
cd agent-brain-server && python -c "import yaml; yaml.safe_load(open('templates/docker-compose.postgres.yml'))" && echo "YAML valid"
```

Verify both template copies are identical:
```bash
diff agent-brain-server/templates/docker-compose.postgres.yml agent-brain-plugin/templates/docker-compose.postgres.yml && echo "Templates match"
```
  </verify>
  <done>Schema manager creates tables with dynamic vector dimensions, validates dimension mismatches, stores/retrieves embedding metadata. Docker Compose template starts pgvector:pg16 with named volume and health check.</done>
</task>

</tasks>

<verification>
All verification for Plan 01:

1. `cd agent-brain-server && poetry run mypy agent_brain_server/storage/postgres/ --strict --ignore-missing-imports` -- No type errors
2. `cd agent-brain-server && poetry run python -c "from agent_brain_server.storage.postgres import PostgresConfig"` -- Import succeeds
3. `cd agent-brain-server && poetry run ruff check agent_brain_server/storage/postgres/` -- No lint errors
4. `cd agent-brain-server && poetry run black --check agent_brain_server/storage/postgres/` -- Formatting correct
5. Docker Compose templates exist and are valid YAML
6. Templates are identical in both locations
</verification>

<success_criteria>
- PostgresConfig with all connection fields, pool config, HNSW params, language validation
- get_connection_url() builds valid asyncpg connection string
- from_database_url() parses DATABASE_URL override
- PostgresConnectionManager with async engine, retry, pool metrics, close
- PostgresSchemaManager with CREATE TABLE SQL, dimension validation, metadata storage
- Docker Compose template with pgvector:pg16, named volume, health check
- All files pass mypy strict (with --ignore-missing-imports for sqlalchemy/asyncpg)
- All files pass ruff and black
</success_criteria>

<output>
After completion, create `.planning/phases/06-postgresql-backend/06-01-SUMMARY.md`
</output>
