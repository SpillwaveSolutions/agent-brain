---
phase: 06-postgresql-backend
plan: 02
type: execute
wave: 2
depends_on: ["06-01"]
files_modified:
  - agent-brain-server/agent_brain_server/storage/postgres/vector_ops.py
  - agent-brain-server/agent_brain_server/storage/postgres/keyword_ops.py
  - agent-brain-server/agent_brain_server/storage/postgres/backend.py
  - agent-brain-server/agent_brain_server/storage/postgres/__init__.py
autonomous: true
must_haves:
  truths:
    - "Vector search supports cosine, L2, and inner product distance metrics"
    - "Vector search returns results with scores normalized to 0-1 (higher=better)"
    - "Keyword search uses tsvector with weighted relevance (title=A, summary=B, content=C)"
    - "Keyword search scores normalized to 0-1 via per-query max normalization"
    - "Keyword search language is configurable (default: english)"
    - "Upsert generates tsvector column with setweight() during insert"
    - "Hybrid search uses RRF (k=60) combining vector and keyword results"
    - "PostgresBackend implements all 11 StorageBackendProtocol methods"
    - "All exceptions wrapped in StorageError with backend='postgres'"
  artifacts:
    - path: "agent-brain-server/agent_brain_server/storage/postgres/vector_ops.py"
      provides: "pgvector vector search and upsert operations"
      exports: ["VectorOps"]
    - path: "agent-brain-server/agent_brain_server/storage/postgres/keyword_ops.py"
      provides: "tsvector keyword search and upsert operations"
      exports: ["KeywordOps"]
    - path: "agent-brain-server/agent_brain_server/storage/postgres/backend.py"
      provides: "PostgresBackend implementing StorageBackendProtocol"
      exports: ["PostgresBackend"]
  key_links:
    - from: "backend.py"
      to: "vector_ops.py"
      via: "VectorOps instance for vector_search and upsert"
      pattern: "self\\.vector_ops\\."
    - from: "backend.py"
      to: "keyword_ops.py"
      via: "KeywordOps instance for keyword_search"
      pattern: "self\\.keyword_ops\\."
    - from: "backend.py"
      to: "connection.py"
      via: "PostgresConnectionManager for all DB operations"
      pattern: "self\\.connection_manager\\."
    - from: "backend.py"
      to: "schema.py"
      via: "PostgresSchemaManager for initialization"
      pattern: "self\\.schema_manager\\."
    - from: "backend.py"
      to: "../protocol.py"
      via: "Implements StorageBackendProtocol (11 methods)"
      pattern: "SearchResult|EmbeddingMetadata|StorageError"
---

<objective>
Implement the PostgreSQL backend core: pgvector vector operations, tsvector keyword operations, and the PostgresBackend class that implements all 11 StorageBackendProtocol methods.

Purpose: This is the heart of Phase 6 -- the actual PostgreSQL storage backend that can be swapped in for ChromaDB. Once this plan completes, PostgresBackend can perform all storage operations.

Output: 3 Python modules (vector_ops, keyword_ops, backend) + updated __init__.py
</objective>

<execution_context>
@/Users/richardhightower/.claude/get-shit-done/workflows/execute-plan.md
@/Users/richardhightower/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/06-postgresql-backend/06-CONTEXT.md
@.planning/phases/06-postgresql-backend/06-RESEARCH.md
@.planning/phases/06-postgresql-backend/06-01-SUMMARY.md
@agent-brain-server/agent_brain_server/storage/protocol.py
@agent-brain-server/agent_brain_server/storage/chroma/backend.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Vector and keyword operations modules</name>
  <files>
    agent-brain-server/agent_brain_server/storage/postgres/vector_ops.py
    agent-brain-server/agent_brain_server/storage/postgres/keyword_ops.py
  </files>
  <action>
**vector_ops.py** -- Create `VectorOps` class for pgvector operations:

**Constructor:** Accept `connection_manager: PostgresConnectionManager`.

**`async upsert_embeddings(chunk_id: str, embedding: list[float]) -> None`:**
Update embedding column for existing document:
```sql
UPDATE documents SET embedding = :embedding, updated_at = NOW() WHERE chunk_id = :chunk_id
```
Use `sqlalchemy.text()` with named parameters. Cast embedding to `::vector` in SQL.

**`async vector_search(query_embedding: list[float], top_k: int, similarity_threshold: float, distance_metric: str = "cosine", where: dict[str, Any] | None = None) -> list[SearchResult]`:**

Support three distance metrics (per requirement PGVEC-02):
- `"cosine"`: Use `<=>` operator (cosine distance). Score = `1 - distance` (normalized to 0-1, higher=better).
- `"l2"`: Use `<->` operator (Euclidean distance). Score = `1 / (1 + distance)` (normalized to 0-1).
- `"inner_product"`: Use `<#>` operator (negative inner product). Score = `0 - distance` (already similarity).

Build SQL:
```sql
SELECT chunk_id, document_text, metadata,
       {distance_expression} as distance
FROM documents
WHERE embedding IS NOT NULL
{AND metadata_filter}
ORDER BY embedding {operator} :query_embedding
LIMIT :top_k
```

For metadata `where` filter: convert dict to JSONB containment query `metadata @> :filter::jsonb`.

Normalize scores to 0-1 per the metric above. Filter results below `similarity_threshold`. Return `list[SearchResult]`.

**keyword_ops.py** -- Create `KeywordOps` class for tsvector operations:

**Constructor:** Accept `connection_manager: PostgresConnectionManager` and `language: str = "english"`.

**`async upsert_with_tsvector(chunk_id: str, document_text: str, metadata: dict[str, Any]) -> None`:**
Build weighted tsvector using `setweight()` (per user decision: "single combined tsvector column with setweight() for relevance boosting"):
```sql
INSERT INTO documents (chunk_id, document_text, metadata, tsv)
VALUES (:chunk_id, :document_text, :metadata::jsonb,
    setweight(to_tsvector(:language, COALESCE(:title, '')), 'A') ||
    setweight(to_tsvector(:language, COALESCE(:summary, '')), 'B') ||
    setweight(to_tsvector(:language, :document_text), 'C')
)
ON CONFLICT (chunk_id) DO UPDATE SET
    document_text = EXCLUDED.document_text,
    metadata = EXCLUDED.metadata,
    tsv = EXCLUDED.tsv,
    updated_at = NOW()
```
Extract title from `metadata.get('filename') or metadata.get('title') or ''`. Extract summary from `metadata.get('summary') or ''`.

**`async keyword_search(query: str, top_k: int, source_types: list[str] | None = None, languages: list[str] | None = None) -> list[SearchResult]`:**
Use `websearch_to_tsquery()` for user-friendly query syntax:
```sql
SELECT chunk_id, document_text, metadata,
       ts_rank(tsv, websearch_to_tsquery(:language, :query)) as score
FROM documents
WHERE tsv @@ websearch_to_tsquery(:language, :query)
{AND source_type_filter}
{AND language_filter}
ORDER BY score DESC
LIMIT :top_k
```

For `source_types` filter: `AND metadata->>'source_type' = ANY(:source_types)`.
For `languages` filter: `AND metadata->>'language' = ANY(:languages)`.

Normalize scores to 0-1 (per-query max normalization, matching ChromaBackend's BM25 approach):
```python
max_score = max(row.score for row in rows) if rows else 1.0
normalized = row.score / max_score if max_score > 0 else 0.0
```

Return `list[SearchResult]`.

Both modules: Use `sqlalchemy.text()` for all SQL. Use `async with engine.begin() as conn` for writes, `async with engine.connect() as conn` for reads. Import `SearchResult` from `..protocol`. All exceptions caught and re-raised as `StorageError(backend="postgres")`. Full type hints. Google-style docstrings.

Use `import json` to serialize metadata dicts to JSON strings for JSONB parameters where needed.
  </action>
  <verify>
```bash
cd agent-brain-server && poetry run mypy agent_brain_server/storage/postgres/vector_ops.py agent_brain_server/storage/postgres/keyword_ops.py --strict --ignore-missing-imports
```
No type errors.

```bash
cd agent-brain-server && poetry run ruff check agent_brain_server/storage/postgres/vector_ops.py agent_brain_server/storage/postgres/keyword_ops.py
```
No lint errors.
  </verify>
  <done>VectorOps supports cosine/L2/inner_product with normalized 0-1 scores. KeywordOps uses weighted tsvector with configurable language and per-query score normalization.</done>
</task>

<task type="auto">
  <name>Task 2: PostgresBackend implementing StorageBackendProtocol</name>
  <files>
    agent-brain-server/agent_brain_server/storage/postgres/backend.py
    agent-brain-server/agent_brain_server/storage/postgres/__init__.py
  </files>
  <action>
**backend.py** -- Create `PostgresBackend` class implementing all 11 `StorageBackendProtocol` methods:

**Constructor:** Accept `config: PostgresConfig`. Create internal instances:
- `self.config = config`
- `self.connection_manager = PostgresConnectionManager(config)`
- `self.schema_manager: PostgresSchemaManager | None = None` (created during initialize when dimensions known)
- `self.vector_ops = VectorOps(self.connection_manager)`
- `self.keyword_ops = KeywordOps(self.connection_manager, language=config.language)`
- `self._initialized = False`

**`is_initialized` property:** Return `self._initialized`.

**`async initialize() -> None`:**
1. Call `self.connection_manager.initialize_with_retry()` (exponential backoff per user decision)
2. Get embedding dimensions from provider config:
   ```python
   provider_settings = load_provider_settings()
   embedding_provider = ProviderRegistry.get_embedding_provider(provider_settings.embedding)
   dimensions = embedding_provider.get_dimensions()
   ```
3. Create `PostgresSchemaManager(self.connection_manager, dimensions, config=self.config)`
4. Call `await self.schema_manager.create_schema()`
5. Call `await self.schema_manager.validate_dimensions()` (fail fast on mismatch per user decision)
6. Set `self._initialized = True`
7. Log: "PostgresBackend initialized (dimensions={dimensions})"

**`async upsert_documents(ids, embeddings, documents, metadatas) -> int`:**
Validate input list lengths match. For each document:
1. Call `keyword_ops.upsert_with_tsvector(id, document, metadata)` (upserts text + tsvector)
2. Call `vector_ops.upsert_embeddings(id, embedding)` (updates embedding column)
Return count of documents upserted.

NOTE: Do this in batches -- iterate through the lists and upsert each document. For MVP, individual upserts are acceptable. Batch optimization can come later.

**`async vector_search(query_embedding, top_k, similarity_threshold, where=None) -> list[SearchResult]`:**
Delegate to `self.vector_ops.vector_search()`.

**`async keyword_search(query, top_k, source_types=None, languages=None) -> list[SearchResult]`:**
Delegate to `self.keyword_ops.keyword_search()`.

**`async hybrid_search_with_rrf(query, query_embedding, top_k, vector_weight=0.5, keyword_weight=0.5, rrf_k=60) -> list[SearchResult]`:**
Implement RRF (Reciprocal Rank Fusion) per research pattern:
1. Fetch `2 * top_k` results from both `vector_search` and `keyword_search`
2. Build RRF scores: `score[id] += weight / (rank + rrf_k)` for each result list
3. Merge results, sort by RRF score descending
4. Normalize RRF scores to 0-1 (divide by max)
5. Return top_k results as `list[SearchResult]`

**`async get_count(where=None) -> int`:**
Execute `SELECT COUNT(*) FROM documents` (with optional JSONB filter). Return count.

**`async get_by_id(chunk_id) -> dict[str, Any] | None`:**
Execute `SELECT document_text, metadata FROM documents WHERE chunk_id = :id`. Return `{"text": ..., "metadata": ...}` or None.

**`async reset() -> None`:**
Execute `DROP TABLE IF EXISTS documents; DROP TABLE IF EXISTS embedding_metadata;`. Then call `await self.schema_manager.create_schema()` to recreate. Log reset.

**`async get_embedding_metadata() -> EmbeddingMetadata | None`:**
Delegate to `self.schema_manager.get_embedding_metadata()`. Convert dict to `EmbeddingMetadata` dataclass if found.

**`async set_embedding_metadata(provider, model, dimensions) -> None`:**
Delegate to `self.schema_manager.store_embedding_metadata()`.

**`validate_embedding_compatibility(provider, model, dimensions, stored_metadata) -> None`:**
Synchronous validation. If `stored_metadata` exists and `stored_metadata.dimensions != dimensions`, raise `ProviderMismatchError` (import from `agent_brain_server.providers.exceptions`). Same logic as ChromaBackend but without delegation.

**`async close() -> None`:**
Call `self.connection_manager.close()`. Set `self._initialized = False`. Log closure.

**__init__.py** -- Update exports to include `PostgresBackend`, `PostgresConfig`, `PostgresConnectionManager`, `PostgresSchemaManager`.

All exceptions caught at method boundaries and re-raised as `StorageError(message, backend="postgres")`. Full type hints. Google-style docstrings.
  </action>
  <verify>
```bash
cd agent-brain-server && poetry run mypy agent_brain_server/storage/postgres/ --strict --ignore-missing-imports
```
No type errors.

```bash
cd agent-brain-server && poetry run ruff check agent_brain_server/storage/postgres/
```
No lint errors.

```bash
cd agent-brain-server && poetry run black --check agent_brain_server/storage/postgres/
```
Formatting correct.
  </verify>
  <done>PostgresBackend implements all 11 StorageBackendProtocol methods plus hybrid_search_with_rrf and close. All operations use async connection pool. All scores normalized to 0-1. All exceptions wrapped in StorageError.</done>
</task>

</tasks>

<verification>
All verification for Plan 02:

1. `cd agent-brain-server && poetry run mypy agent_brain_server/storage/postgres/ --strict --ignore-missing-imports` -- No type errors across all postgres modules
2. `cd agent-brain-server && poetry run ruff check agent_brain_server/storage/postgres/` -- No lint errors
3. `cd agent-brain-server && poetry run black --check agent_brain_server/storage/postgres/` -- Formatting correct
4. `cd agent-brain-server && poetry run python -c "from agent_brain_server.storage.postgres import PostgresBackend, PostgresConfig"` -- Imports succeed
5. Verify PostgresBackend has all 11 protocol methods:
```bash
cd agent-brain-server && poetry run python -c "
from agent_brain_server.storage.postgres.backend import PostgresBackend
methods = ['initialize', 'upsert_documents', 'vector_search', 'keyword_search', 'get_count', 'get_by_id', 'reset', 'get_embedding_metadata', 'set_embedding_metadata', 'validate_embedding_compatibility', 'is_initialized']
for m in methods:
    assert hasattr(PostgresBackend, m), f'Missing: {m}'
print('All 11 protocol methods present')
"
```
</verification>

<success_criteria>
- VectorOps supports cosine, L2, inner_product distance metrics with score normalization
- KeywordOps uses weighted tsvector (A/B/C) with configurable language
- PostgresBackend implements all 11 StorageBackendProtocol methods
- hybrid_search_with_rrf uses RRF fusion (k=60) with normalized output
- All SQL uses parameterized queries (no string concatenation for user input)
- All exceptions wrapped in StorageError
- All files pass mypy strict, ruff, black
</success_criteria>

<output>
After completion, create `.planning/phases/06-postgresql-backend/06-02-SUMMARY.md`
</output>
