---
phase: 06-postgresql-backend
plan: 03
type: execute
wave: 3
depends_on: ["06-02"]
files_modified:
  - agent-brain-server/agent_brain_server/storage/factory.py
  - agent-brain-server/agent_brain_server/storage/__init__.py
  - agent-brain-server/agent_brain_server/config/provider_config.py
  - agent-brain-server/agent_brain_server/api/routers/health.py
  - agent-brain-server/agent_brain_server/api/main.py
  - agent-brain-server/pyproject.toml
  - agent-brain-server/tests/unit/storage/test_postgres_config.py
  - agent-brain-server/tests/unit/storage/test_postgres_connection.py
  - agent-brain-server/tests/unit/storage/test_postgres_schema.py
  - agent-brain-server/tests/unit/storage/test_postgres_backend.py
  - agent-brain-server/tests/unit/storage/test_postgres_vector_ops.py
  - agent-brain-server/tests/unit/storage/test_postgres_keyword_ops.py
  - agent-brain-server/tests/unit/api/test_health_postgres.py
autonomous: true
must_haves:
  truths:
    - "get_storage_backend() returns PostgresBackend when backend='postgres'"
    - "StorageConfig.postgres dict is parsed into PostgresConfig via factory"
    - "DATABASE_URL env var overrides YAML postgres config in factory"
    - "/health/postgres endpoint returns pool metrics when backend is postgres"
    - "/health/postgres returns 400 when backend is not postgres"
    - "Server lifespan calls backend.close() on shutdown for PostgreSQL"
    - "Poetry extras [postgres] installs asyncpg and sqlalchemy"
    - "All existing tests still pass (zero regression)"
    - "Unit tests cover PostgresConfig, connection, schema, backend, vector_ops, keyword_ops"
  artifacts:
    - path: "agent-brain-server/agent_brain_server/storage/factory.py"
      provides: "Factory creates PostgresBackend from config"
      contains: "PostgresBackend"
    - path: "agent-brain-server/agent_brain_server/api/routers/health.py"
      provides: "/health/postgres endpoint with pool metrics"
      contains: "postgres_health"
    - path: "agent-brain-server/pyproject.toml"
      provides: "Poetry extras for PostgreSQL dependencies"
      contains: "postgres"
    - path: "agent-brain-server/tests/unit/storage/test_postgres_config.py"
      provides: "PostgresConfig unit tests"
    - path: "agent-brain-server/tests/unit/storage/test_postgres_backend.py"
      provides: "PostgresBackend unit tests with mocked DB"
  key_links:
    - from: "factory.py"
      to: "postgres/backend.py"
      via: "Creates PostgresBackend when backend_type='postgres'"
      pattern: "PostgresBackend"
    - from: "factory.py"
      to: "postgres/config.py"
      via: "Parses StorageConfig.postgres dict into PostgresConfig"
      pattern: "PostgresConfig"
    - from: "health.py"
      to: "postgres/connection.py"
      via: "Reads pool metrics from connection manager"
      pattern: "get_pool_status"
    - from: "main.py"
      to: "postgres/backend.py"
      via: "Calls close() on shutdown via lifespan"
      pattern: "backend\\.close"
---

<objective>
Wire PostgresBackend into the server: update factory to create PostgresBackend, add /health/postgres endpoint, update lifespan for pool lifecycle, add Poetry extras, and write comprehensive unit tests for all postgres modules.

Purpose: This plan completes Phase 6 by connecting the PostgreSQL backend to the rest of the application and ensuring quality through tests. After this plan, switching `storage.backend: postgres` in config.yaml activates the full PostgreSQL backend.

Output: 6 modified server files + 7 new test files
</objective>

<execution_context>
@/Users/richardhightower/.claude/get-shit-done/workflows/execute-plan.md
@/Users/richardhightower/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/06-postgresql-backend/06-CONTEXT.md
@.planning/phases/06-postgresql-backend/06-RESEARCH.md
@.planning/phases/06-postgresql-backend/06-01-SUMMARY.md
@.planning/phases/06-postgresql-backend/06-02-SUMMARY.md
@agent-brain-server/agent_brain_server/storage/protocol.py
@agent-brain-server/agent_brain_server/storage/factory.py
@agent-brain-server/agent_brain_server/storage/__init__.py
@agent-brain-server/agent_brain_server/config/provider_config.py
@agent-brain-server/agent_brain_server/api/routers/health.py
@agent-brain-server/agent_brain_server/api/main.py
@agent-brain-server/pyproject.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Factory integration, health endpoint, lifespan, and Poetry extras</name>
  <files>
    agent-brain-server/agent_brain_server/storage/factory.py
    agent-brain-server/agent_brain_server/storage/__init__.py
    agent-brain-server/agent_brain_server/config/provider_config.py
    agent-brain-server/agent_brain_server/api/routers/health.py
    agent-brain-server/agent_brain_server/api/main.py
    agent-brain-server/pyproject.toml
  </files>
  <action>
**factory.py** -- Replace the `NotImplementedError` for postgres backend with actual creation:

In the `elif backend_type == "postgres":` branch:
1. Import `PostgresBackend` and `PostgresConfig` from `agent_brain_server.storage.postgres` (lazy import inside the branch to avoid importing asyncpg when using chroma)
2. Load postgres config from YAML: `provider_settings = load_provider_settings()` then `postgres_dict = provider_settings.storage.postgres`
3. Check for DATABASE_URL env var override: `database_url = os.getenv("DATABASE_URL")`. If set, use `PostgresConfig.from_database_url(database_url)` to create config, but preserve pool_size/pool_max_overflow from YAML dict (per user decision: "DATABASE_URL overrides connection string only, pool config stays in YAML").
4. If no DATABASE_URL: `config = PostgresConfig(**postgres_dict)` -- parse YAML dict directly into Pydantic model.
5. Create `_storage_backend = PostgresBackend(config=config)`
6. Set `_backend_type = backend_type`
7. Return `_storage_backend`

Add `import os` to imports if not present.

**provider_config.py** -- In `validate_provider_config()`, update the postgres validation block to validate more specifically when backend is postgres:
- If `settings.storage.postgres` has values, validate that at minimum `host` key exists
- Keep existing WARNING for empty postgres config

**health.py** -- Add new `/health/postgres` endpoint to the existing health router:

```python
@router.get("/postgres")
async def postgres_health(request: Request) -> dict[str, Any]:
```

Implementation (per user decision: "dedicated /health/postgres endpoint with pool metrics"):
1. Check `get_effective_backend_type()` -- if not "postgres", raise `HTTPException(status_code=400, detail="PostgreSQL health endpoint only available when storage backend is 'postgres'")`
2. Get backend from `request.app.state.storage_backend`
3. Try to get pool metrics via `backend.connection_manager.get_pool_status()`
4. Try a test query: `SELECT version()` via `backend.connection_manager.engine`
5. Return healthy response: `{"status": "healthy", "backend": "postgres", "pool": pool_metrics, "database": {"version": version, "host": config.host, "port": config.port, "database": config.database}}`
6. On exception: return `{"status": "unhealthy", "backend": "postgres", "error": str(e)}`

Add necessary imports: `from typing import Any`, `from fastapi import HTTPException`. Import `get_effective_backend_type` from storage.

**main.py** -- Update `lifespan()` to handle PostgreSQL backend lifecycle:

In the shutdown section (after `yield`), add pool cleanup:
```python
# Close storage backend if it has a close method (PostgreSQL pool)
storage_backend = getattr(app.state, "storage_backend", None)
if storage_backend and hasattr(storage_backend, "close"):
    await storage_backend.close()
    logger.info("Storage backend connection pool closed")
```

This is safe for ChromaBackend (no close method). Only PostgresBackend has close().

**pyproject.toml** -- Add PostgreSQL optional dependencies (per user decision: "PostgreSQL dependencies as Poetry extras"):

In `[tool.poetry.dependencies]`:
```toml
asyncpg = {version = "^0.29.0", optional = true}
sqlalchemy = {version = "^2.0.0", extras = ["asyncio"], optional = true}
```

In `[tool.poetry.extras]`:
```toml
postgres = ["asyncpg", "sqlalchemy"]
```

Add pytest marker for postgres tests:
```toml
"postgres: marks tests that require PostgreSQL database",
```

Do NOT add psycopg2-binary -- not needed with asyncpg. Keep it simple per research recommendation.

**__init__.py** -- No changes needed (PostgresBackend imported lazily in factory).
  </action>
  <verify>
```bash
cd agent-brain-server && poetry run mypy agent_brain_server/storage/factory.py --strict
```

```bash
cd agent-brain-server && poetry run mypy agent_brain_server/api/routers/health.py --strict
```

```bash
cd agent-brain-server && poetry run ruff check agent_brain_server/storage/factory.py agent_brain_server/api/routers/health.py agent_brain_server/api/main.py agent_brain_server/config/provider_config.py
```

Verify pyproject.toml is valid:
```bash
cd agent-brain-server && poetry check
```

Verify existing tests still pass:
```bash
cd agent-brain-server && poetry run pytest tests/ -x -q --timeout=60
```
  </verify>
  <done>Factory creates PostgresBackend from YAML config with DATABASE_URL override. /health/postgres returns pool metrics. Lifespan closes pool on shutdown. Poetry extras installable via `poetry install -E postgres`.</done>
</task>

<task type="auto">
  <name>Task 2: Unit tests for PostgreSQL config, connection, and schema</name>
  <files>
    agent-brain-server/tests/unit/storage/test_postgres_config.py
    agent-brain-server/tests/unit/storage/test_postgres_connection.py
    agent-brain-server/tests/unit/storage/test_postgres_schema.py
  </files>
  <action>
All tests use mocks -- NO real PostgreSQL database needed. Tests should NOT have the `@pytest.mark.postgres` marker (they are pure unit tests with mocks).

**test_postgres_config.py** (target: 12-15 tests):

Test PostgresConfig model:
- Default values (host=localhost, port=5432, database=agent_brain, etc.)
- `get_connection_url()` builds correct asyncpg URL
- `get_connection_url()` with password URL-encodes special characters (test with `p@ss:word/123`)
- `get_connection_url()` without password omits password from URL
- `from_database_url()` parses standard PostgreSQL URLs
- `from_database_url()` parses URL with port and database
- Language validation accepts supported languages (english, spanish, etc.)
- Language validation rejects unsupported languages (raises ValueError)
- Language validation is case-insensitive ("ENGLISH" -> "english")
- Port validation rejects invalid range (0, 65536)
- Pool size and overflow defaults are 10 each
- HNSW defaults (m=16, ef_construction=64)
- Custom field values override defaults

**test_postgres_connection.py** (target: 8-10 tests):

Mock `create_async_engine` from `sqlalchemy.ext.asyncio`. All tests patch this.

Test PostgresConnectionManager:
- `initialize()` creates engine with correct parameters (pool_size, max_overflow, pre_ping, recycle)
- `initialize()` uses connection URL from config
- `close()` disposes engine
- `close()` is idempotent (safe to call twice)
- `engine` property raises RuntimeError when not initialized
- `engine` property returns engine after initialization
- `get_pool_status()` returns metrics dict (mock pool with size/checkedin/checkedout/overflow methods)
- `get_pool_status()` returns not_initialized when engine is None
- `initialize_with_retry()` succeeds on first attempt when DB is available
- `initialize_with_retry()` retries on failure and succeeds eventually (mock initialize to fail then succeed)
- `initialize_with_retry()` raises StorageError after max_attempts

Mock `asyncio.sleep` to avoid actual delays in retry tests.

**test_postgres_schema.py** (target: 8-10 tests):

Mock the connection manager engine. All SQL execution via mocked `conn.execute()`.

Test PostgresSchemaManager:
- `create_schema()` executes SQL containing "CREATE EXTENSION IF NOT EXISTS vector"
- `create_schema()` SQL contains correct vector dimension (e.g., `vector(3072)`)
- `create_schema()` SQL contains HNSW index with correct parameters
- `create_schema()` SQL contains GIN index for tsvector
- `create_schema()` SQL creates embedding_metadata table
- `validate_dimensions()` passes when dimensions match
- `validate_dimensions()` raises StorageError on dimension mismatch
- `validate_dimensions()` passes when no metadata exists (first run)
- `store_embedding_metadata()` executes INSERT/ON CONFLICT SQL
- `get_embedding_metadata()` returns dict when metadata exists
- `get_embedding_metadata()` returns None when no metadata exists

Use `unittest.mock.AsyncMock` for async context managers. Create helper to mock `async with engine.begin() as conn` and `async with engine.connect() as conn` patterns.
  </action>
  <verify>
```bash
cd agent-brain-server && poetry run pytest tests/unit/storage/test_postgres_config.py tests/unit/storage/test_postgres_connection.py tests/unit/storage/test_postgres_schema.py -v --timeout=30
```
All tests pass.

```bash
cd agent-brain-server && poetry run mypy tests/unit/storage/test_postgres_config.py tests/unit/storage/test_postgres_connection.py tests/unit/storage/test_postgres_schema.py --strict --ignore-missing-imports
```
  </verify>
  <done>30+ unit tests covering PostgresConfig validation, connection pool lifecycle, retry logic, schema creation SQL, and dimension validation -- all with mocks, no real database needed.</done>
</task>

<task type="auto">
  <name>Task 3: Unit tests for backend, vector ops, keyword ops, and health endpoint</name>
  <files>
    agent-brain-server/tests/unit/storage/test_postgres_backend.py
    agent-brain-server/tests/unit/storage/test_postgres_vector_ops.py
    agent-brain-server/tests/unit/storage/test_postgres_keyword_ops.py
    agent-brain-server/tests/unit/api/test_health_postgres.py
  </files>
  <action>
All tests use mocks -- NO real PostgreSQL database needed. No `@pytest.mark.postgres` marker.

**test_postgres_vector_ops.py** (target: 8-10 tests):

Mock connection manager engine. Test VectorOps:
- `upsert_embeddings()` executes UPDATE SQL with correct chunk_id and embedding
- `vector_search()` with cosine metric uses `<=>` operator
- `vector_search()` with l2 metric uses `<->` operator
- `vector_search()` with inner_product metric uses `<#>` operator
- `vector_search()` normalizes cosine scores to 0-1 (1 - distance)
- `vector_search()` normalizes L2 scores to 0-1 (1/(1+distance))
- `vector_search()` returns empty list when no results
- `vector_search()` filters by similarity_threshold
- `vector_search()` applies metadata where filter
- `vector_search()` wraps exceptions in StorageError

**test_postgres_keyword_ops.py** (target: 8-10 tests):

Mock connection manager engine. Test KeywordOps:
- `upsert_with_tsvector()` executes INSERT with weighted setweight() SQL
- `upsert_with_tsvector()` extracts title from metadata.filename
- `upsert_with_tsvector()` extracts summary from metadata.summary
- `keyword_search()` uses websearch_to_tsquery with configured language
- `keyword_search()` normalizes scores to 0-1 (per-query max)
- `keyword_search()` returns empty list when no matches
- `keyword_search()` handles zero scores correctly
- `keyword_search()` filters by source_types
- `keyword_search()` filters by languages
- `keyword_search()` wraps exceptions in StorageError

**test_postgres_backend.py** (target: 15-18 tests):

Mock PostgresConnectionManager, PostgresSchemaManager, VectorOps, KeywordOps. Test PostgresBackend:
- Constructor creates all internal components
- `is_initialized` returns False before initialize, True after
- `initialize()` calls connection_manager.initialize_with_retry()
- `initialize()` calls schema_manager.create_schema()
- `initialize()` calls schema_manager.validate_dimensions()
- `initialize()` raises StorageError on connection failure
- `upsert_documents()` calls keyword_ops and vector_ops for each document
- `upsert_documents()` validates list lengths match
- `upsert_documents()` returns correct count
- `vector_search()` delegates to vector_ops
- `keyword_search()` delegates to keyword_ops
- `hybrid_search_with_rrf()` combines vector and keyword results
- `hybrid_search_with_rrf()` normalizes RRF scores to 0-1
- `get_count()` executes COUNT query
- `get_by_id()` returns document dict or None
- `reset()` drops and recreates tables
- `get/set_embedding_metadata()` delegate to schema_manager
- `validate_embedding_compatibility()` raises on dimension mismatch
- `close()` calls connection_manager.close()

For `hybrid_search_with_rrf` test: mock `vector_search` to return 3 results and `keyword_search` to return 3 results (some overlapping chunk_ids). Verify merged results have RRF scores, are sorted descending, and normalized to 0-1.

**test_health_postgres.py** (target: 4-5 tests):

Use FastAPI TestClient with httpx. Mock app.state.storage_backend.

- `/health/postgres` returns 200 with pool metrics when backend is postgres
- `/health/postgres` returns pool_size, checked_in, checked_out, overflow, total
- `/health/postgres` returns database version and connection info
- `/health/postgres` returns 400 when backend is not postgres
- `/health/postgres` returns unhealthy status on connection error

Use `unittest.mock.patch` to mock `get_effective_backend_type()` return value.

After all test files are written, run the FULL test suite to verify zero regression:
```bash
cd agent-brain-server && poetry run pytest tests/ -x -q --timeout=60
```

Then run formatting and linting on all new test files:
```bash
cd agent-brain-server && poetry run black tests/unit/storage/test_postgres_*.py tests/unit/api/test_health_postgres.py
cd agent-brain-server && poetry run ruff check tests/unit/storage/test_postgres_*.py tests/unit/api/test_health_postgres.py --fix
```
  </action>
  <verify>
```bash
cd agent-brain-server && poetry run pytest tests/unit/storage/test_postgres_backend.py tests/unit/storage/test_postgres_vector_ops.py tests/unit/storage/test_postgres_keyword_ops.py tests/unit/api/test_health_postgres.py -v --timeout=30
```
All new tests pass.

Full suite regression check:
```bash
cd agent-brain-server && poetry run pytest tests/ -x -q --timeout=120
```
559+ tests pass (existing 559 + new ~50-60 postgres tests).

Quality gate:
```bash
cd agent-brain-server && poetry run black --check agent_brain_server/ tests/ && poetry run ruff check agent_brain_server/ tests/ && poetry run mypy agent_brain_server/ --strict --ignore-missing-imports
```
  </verify>
  <done>50+ unit tests covering all PostgreSQL backend modules. Full test suite passes with zero regression. All new code passes mypy strict, ruff, and black.</done>
</task>

</tasks>

<verification>
All verification for Plan 03:

1. Factory creates PostgresBackend:
```bash
cd agent-brain-server && poetry run python -c "
from unittest.mock import patch
import os
with patch.dict(os.environ, {'AGENT_BRAIN_STORAGE_BACKEND': 'postgres'}):
    from agent_brain_server.storage.factory import get_effective_backend_type, reset_storage_backend_cache
    reset_storage_backend_cache()
    bt = get_effective_backend_type()
    assert bt == 'postgres', f'Expected postgres, got {bt}'
    print('Factory resolves postgres backend type correctly')
"
```

2. Health endpoint exists:
```bash
cd agent-brain-server && poetry run python -c "
from agent_brain_server.api.routers.health import router
routes = [r.path for r in router.routes]
assert '/postgres' in routes, f'Missing /postgres route. Routes: {routes}'
print('/health/postgres endpoint registered')
"
```

3. Poetry extras defined:
```bash
cd agent-brain-server && poetry run python -c "
import tomllib
with open('pyproject.toml', 'rb') as f:
    data = tomllib.load(f)
extras = data['tool']['poetry']['extras']
assert 'postgres' in extras, f'Missing postgres extra. Extras: {extras}'
assert 'asyncpg' in extras['postgres'], 'Missing asyncpg in postgres extra'
assert 'sqlalchemy' in extras['postgres'], 'Missing sqlalchemy in postgres extra'
print('Poetry postgres extras: OK')
"
```

4. Full test suite passes with zero regression:
```bash
cd agent-brain-server && poetry run pytest tests/ -x -q --timeout=120
```
Target: 600+ tests passing (559 existing + 50-60 new postgres tests)

5. Full quality gate:
```bash
cd agent-brain-server && poetry run black --check agent_brain_server/ tests/ && poetry run ruff check agent_brain_server/ tests/ && poetry run mypy agent_brain_server/ --strict --ignore-missing-imports && echo "QUALITY GATE PASSED"
```
</verification>

<success_criteria>
- `storage.backend: postgres` in config.yaml activates PostgresBackend via factory
- DATABASE_URL env var overrides YAML connection string in factory
- /health/postgres returns pool metrics (pool_size, checked_in, checked_out, overflow)
- /health/postgres returns 400 when backend is not postgres
- Server lifespan closes PostgreSQL connection pool on shutdown
- Poetry extras: `poetry install -E postgres` installs asyncpg + sqlalchemy
- 50-60 new unit tests covering all 6 postgres modules + health endpoint
- Zero regression -- all 559+ existing tests still pass
- All code passes mypy strict, ruff, and black
</success_criteria>

<output>
After completion, create `.planning/phases/06-postgresql-backend/06-03-SUMMARY.md`
</output>
