---
phase: 02-pluggable-providers
plan: 03
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - e2e/integration/test_provider_switching.py
  - agent-brain-cli/agent_brain_cli/commands/config.py
  - agent-brain-cli/agent_brain_cli/cli.py
  - e2e/fixtures/config_openai.yaml
  - e2e/fixtures/config_ollama.yaml
autonomous: true

must_haves:
  truths:
    - "E2E test proves provider switching works by changing config file"
    - "Test verifies different embedding dimensions after provider switch"
    - "agent-brain config show CLI command displays active configuration"
    - "Test uses mocked providers to avoid actual API calls"
  artifacts:
    - path: "e2e/integration/test_provider_switching.py"
      provides: "E2E test for provider switching"
      contains: "test_provider_switch"
    - path: "agent-brain-cli/agent_brain_cli/commands/config.py"
      provides: "config show command"
      contains: "def show"
  key_links:
    - from: "agent_brain_cli/cli.py"
      to: "commands/config.py"
      via: "config command group import"
      pattern: "from.*commands.config.*import"
---

<objective>
Create E2E test proving provider switching works and add CLI command for configuration debugging.

Purpose: Verify that changing config.yaml and restarting server uses the new provider (PROV-03 verification). Provide `agent-brain config show` for users to debug which config is active.

Output: E2E test for provider switching, test fixtures for different providers, and CLI config show command.
</objective>

<execution_context>
@/Users/richardhightower/.claude/get-shit-done/workflows/execute-plan.md
@/Users/richardhightower/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/02-pluggable-providers/02-RESEARCH.md
@.planning/phases/02-pluggable-providers/02-01-PLAN.md
@agent-brain-server/agent_brain_server/config/provider_config.py
@agent-brain-cli/agent_brain_cli/cli.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create test fixtures for different provider configurations</name>
  <files>e2e/fixtures/config_openai.yaml, e2e/fixtures/config_ollama.yaml</files>
  <action>
1. Create the e2e/fixtures directory if it doesn't exist:

```bash
mkdir -p e2e/fixtures
```

2. Create `e2e/fixtures/config_openai.yaml`:

```yaml
# OpenAI provider configuration for E2E testing
embedding:
  provider: openai
  model: text-embedding-3-large
  api_key_env: OPENAI_API_KEY

summarization:
  provider: anthropic
  model: claude-haiku-4-5-20251001
  api_key_env: ANTHROPIC_API_KEY
```

3. Create `e2e/fixtures/config_ollama.yaml`:

```yaml
# Ollama provider configuration for E2E testing (fully offline)
embedding:
  provider: ollama
  model: nomic-embed-text
  base_url: http://localhost:11434/v1

summarization:
  provider: ollama
  model: llama3.2
  base_url: http://localhost:11434/v1
```

4. Create `e2e/fixtures/config_cohere.yaml` for dimension testing:

```yaml
# Cohere provider configuration (different dimensions than OpenAI)
embedding:
  provider: cohere
  model: embed-english-v3.0
  api_key_env: COHERE_API_KEY

summarization:
  provider: anthropic
  model: claude-haiku-4-5-20251001
  api_key_env: ANTHROPIC_API_KEY
```
  </action>
  <verify>Run `ls -la e2e/fixtures/` to verify fixture files exist.</verify>
  <done>Test fixture YAML files created for OpenAI, Ollama, and Cohere configurations.</done>
</task>

<task type="auto">
  <name>Task 2: Create E2E test for provider switching</name>
  <files>e2e/integration/test_provider_switching.py</files>
  <action>
Create the test file:

```python
"""E2E tests for provider switching (PROV-03 verification).

These tests verify that changing config.yaml and restarting the server
results in using the new provider configuration.
"""

import os
import shutil
import tempfile
from pathlib import Path
from typing import Any, Generator
from unittest.mock import AsyncMock, MagicMock, patch

import pytest

from agent_brain_server.config.provider_config import (
    clear_settings_cache,
    load_provider_settings,
    _find_config_file,
)
from agent_brain_server.providers.base import EmbeddingProviderType


# Path to fixture files
FIXTURES_DIR = Path(__file__).parent.parent / "fixtures"


@pytest.fixture
def temp_project_dir() -> Generator[Path, None, None]:
    """Create a temporary project directory with .claude/agent-brain structure."""
    with tempfile.TemporaryDirectory() as tmpdir:
        project_dir = Path(tmpdir)
        config_dir = project_dir / ".claude" / "agent-brain"
        config_dir.mkdir(parents=True)
        yield project_dir


@pytest.fixture(autouse=True)
def clear_config_cache() -> Generator[None, None, None]:
    """Clear the provider settings cache before and after each test."""
    clear_settings_cache()
    yield
    clear_settings_cache()


class TestConfigFileDiscovery:
    """Tests for configuration file discovery."""

    def test_finds_config_in_project_dir(self, temp_project_dir: Path) -> None:
        """Test that config is found in .claude/agent-brain/config.yaml."""
        # Copy OpenAI config to project directory
        config_path = temp_project_dir / ".claude" / "agent-brain" / "config.yaml"
        shutil.copy(FIXTURES_DIR / "config_openai.yaml", config_path)

        # Set CWD to project directory
        original_cwd = os.getcwd()
        try:
            os.chdir(temp_project_dir)
            found = _find_config_file()
            assert found is not None
            assert found == config_path
        finally:
            os.chdir(original_cwd)

    def test_env_var_override(self, temp_project_dir: Path) -> None:
        """Test AGENT_BRAIN_CONFIG env var overrides file search."""
        config_path = temp_project_dir / "custom_config.yaml"
        shutil.copy(FIXTURES_DIR / "config_ollama.yaml", config_path)

        with patch.dict(os.environ, {"AGENT_BRAIN_CONFIG": str(config_path)}):
            clear_settings_cache()
            found = _find_config_file()
            assert found == config_path


class TestProviderSwitching:
    """Tests for provider switching behavior (PROV-03)."""

    def test_switch_from_openai_to_ollama(self, temp_project_dir: Path) -> None:
        """Test switching from OpenAI to Ollama provider."""
        config_path = temp_project_dir / ".claude" / "agent-brain" / "config.yaml"

        original_cwd = os.getcwd()
        try:
            os.chdir(temp_project_dir)

            # Start with OpenAI config
            shutil.copy(FIXTURES_DIR / "config_openai.yaml", config_path)
            clear_settings_cache()

            settings1 = load_provider_settings()
            assert settings1.embedding.provider == EmbeddingProviderType.OPENAI
            assert settings1.embedding.model == "text-embedding-3-large"

            # Switch to Ollama config
            shutil.copy(FIXTURES_DIR / "config_ollama.yaml", config_path)
            clear_settings_cache()

            settings2 = load_provider_settings()
            assert settings2.embedding.provider == EmbeddingProviderType.OLLAMA
            assert settings2.embedding.model == "nomic-embed-text"

        finally:
            os.chdir(original_cwd)

    def test_dimension_mismatch_detection(self, temp_project_dir: Path) -> None:
        """Test that dimension mismatch is detected after provider switch."""
        from agent_brain_server.storage.vector_store import (
            EmbeddingMetadata,
            VectorStoreManager,
        )
        from agent_brain_server.providers.exceptions import ProviderMismatchError

        store = VectorStoreManager()

        # Simulate existing OpenAI index (3072 dimensions)
        stored_metadata = EmbeddingMetadata(
            provider="openai",
            model="text-embedding-3-large",
            dimensions=3072,
        )

        # Try to use Ollama (768 dimensions) - should raise
        with pytest.raises(ProviderMismatchError) as exc_info:
            store.validate_embedding_compatibility(
                provider="ollama",
                model="nomic-embed-text",
                dimensions=768,
                stored_metadata=stored_metadata,
            )

        error = exc_info.value
        assert "openai" in str(error)
        assert "ollama" in str(error)
        assert "text-embedding-3-large" in str(error)


class TestProviderInstantiation:
    """Tests for provider instantiation from config."""

    def test_openai_provider_created(self, temp_project_dir: Path) -> None:
        """Test OpenAI provider is created from config."""
        config_path = temp_project_dir / ".claude" / "agent-brain" / "config.yaml"
        shutil.copy(FIXTURES_DIR / "config_openai.yaml", config_path)

        original_cwd = os.getcwd()
        try:
            os.chdir(temp_project_dir)
            clear_settings_cache()

            # Mock the API key
            with patch.dict(os.environ, {"OPENAI_API_KEY": "sk-test-key"}):
                settings = load_provider_settings()
                from agent_brain_server.providers.factory import ProviderRegistry

                # This should create OpenAI provider (mocked)
                with patch(
                    "agent_brain_server.providers.embedding.openai.openai"
                ) as mock_openai:
                    provider = ProviderRegistry.get_embedding_provider(settings.embedding)
                    assert provider.provider_name == "openai"
                    assert provider.get_dimensions() == 3072

        finally:
            os.chdir(original_cwd)

    def test_ollama_provider_no_api_key_needed(self, temp_project_dir: Path) -> None:
        """Test Ollama provider doesn't require API key."""
        config_path = temp_project_dir / ".claude" / "agent-brain" / "config.yaml"
        shutil.copy(FIXTURES_DIR / "config_ollama.yaml", config_path)

        original_cwd = os.getcwd()
        try:
            os.chdir(temp_project_dir)
            clear_settings_cache()

            settings = load_provider_settings()

            # Ollama config should not need API key
            assert settings.embedding.get_api_key() is None
            assert settings.summarization.get_api_key() is None

        finally:
            os.chdir(original_cwd)


class TestConfigShowCommand:
    """Tests for agent-brain config show CLI command."""

    def test_config_show_displays_active_config(
        self, temp_project_dir: Path
    ) -> None:
        """Test config show command displays the active configuration."""
        from click.testing import CliRunner
        from agent_brain_cli.commands.config import show_config

        config_path = temp_project_dir / ".claude" / "agent-brain" / "config.yaml"
        shutil.copy(FIXTURES_DIR / "config_openai.yaml", config_path)

        runner = CliRunner()

        original_cwd = os.getcwd()
        try:
            os.chdir(temp_project_dir)
            clear_settings_cache()

            with patch.dict(os.environ, {"OPENAI_API_KEY": "sk-test"}):
                result = runner.invoke(show_config)
                assert result.exit_code == 0
                assert "openai" in result.output.lower()
                assert "text-embedding-3-large" in result.output

        finally:
            os.chdir(original_cwd)
```
  </action>
  <verify>Run `cd agent-brain-server && poetry run pytest e2e/integration/test_provider_switching.py -v --tb=short` to run the tests (some may fail if providers not mocked correctly; adjust as needed).</verify>
  <done>E2E test file created with tests for provider switching, dimension mismatch detection, and config show command.</done>
</task>

<task type="auto">
  <name>Task 3: Create config CLI command module</name>
  <files>agent-brain-cli/agent_brain_cli/commands/config.py</files>
  <action>
Create the config command module:

```python
"""Config commands for viewing and managing Agent Brain configuration."""

import json
import os
from pathlib import Path
from typing import Any, Optional

import click
from rich.console import Console
from rich.panel import Panel
from rich.table import Table

console = Console()


def _find_config_file() -> Optional[Path]:
    """Find the configuration file in standard locations.

    Search order:
    1. AGENT_BRAIN_CONFIG environment variable
    2. State directory config.yaml (if AGENT_BRAIN_STATE_DIR set)
    3. Current directory config.yaml
    4. Walk up from CWD looking for .claude/agent-brain/config.yaml
    5. User home ~/.agent-brain/config.yaml
    6. XDG config ~/.config/agent-brain/config.yaml

    Returns:
        Path to config file or None if not found
    """
    # 1. Environment variable override
    env_config = os.getenv("AGENT_BRAIN_CONFIG")
    if env_config:
        path = Path(env_config)
        if path.exists():
            return path

    # 2. State directory
    state_dir = os.getenv("AGENT_BRAIN_STATE_DIR") or os.getenv("DOC_SERVE_STATE_DIR")
    if state_dir:
        state_config = Path(state_dir) / "config.yaml"
        if state_config.exists():
            return state_config

    # 3. Current directory
    cwd_config = Path.cwd() / "config.yaml"
    if cwd_config.exists():
        return cwd_config

    # 4. Walk up from CWD
    current = Path.cwd()
    root = Path(current.anchor)
    while current != root:
        claude_config = current / ".claude" / "agent-brain" / "config.yaml"
        if claude_config.exists():
            return claude_config
        current = current.parent

    # 5. User home
    home_config = Path.home() / ".agent-brain" / "config.yaml"
    if home_config.exists():
        return home_config

    # 6. XDG config
    xdg_config = Path.home() / ".config" / "agent-brain" / "config.yaml"
    if xdg_config.exists():
        return xdg_config

    return None


def _load_yaml(path: Path) -> dict[str, Any]:
    """Load YAML configuration file."""
    import yaml

    with open(path) as f:
        return yaml.safe_load(f) or {}


@click.group("config")
def config_group() -> None:
    """View and manage Agent Brain configuration.

    \b
    Commands:
      show   - Display active configuration
      path   - Show config file location
    """
    pass


@config_group.command("show")
@click.option("--json", "json_output", is_flag=True, help="Output as JSON")
def show_config(json_output: bool) -> None:
    """Display the active provider configuration.

    Shows which config file is being used and the current provider settings
    for embedding, summarization, and reranking.

    \b
    Examples:
      agent-brain config show           # Rich formatted output
      agent-brain config show --json    # JSON output for scripting
    """
    config_path = _find_config_file()

    if json_output:
        output: dict[str, Any] = {
            "config_file": str(config_path) if config_path else None,
            "config_source": "file" if config_path else "defaults",
        }

        if config_path:
            config = _load_yaml(config_path)
            output["embedding"] = config.get("embedding", {})
            output["summarization"] = config.get("summarization", {})
            output["reranker"] = config.get("reranker", {})
        else:
            output["embedding"] = {
                "provider": "openai",
                "model": "text-embedding-3-large",
            }
            output["summarization"] = {
                "provider": "anthropic",
                "model": "claude-haiku-4-5-20251001",
            }

        click.echo(json.dumps(output, indent=2))
        return

    # Rich formatted output
    if config_path:
        console.print(f"\n[bold]Config file:[/] {config_path}\n")
        config = _load_yaml(config_path)
    else:
        console.print(
            "\n[yellow]No config file found, using defaults[/]\n"
        )
        config = {}

    # Embedding provider
    embedding = config.get("embedding", {})
    embed_table = Table(title="Embedding Provider", show_header=False)
    embed_table.add_column("Setting", style="cyan")
    embed_table.add_column("Value")
    embed_table.add_row("Provider", embedding.get("provider", "openai"))
    embed_table.add_row("Model", embedding.get("model", "text-embedding-3-large"))
    embed_table.add_row("API Key Env", embedding.get("api_key_env", "OPENAI_API_KEY"))
    if embedding.get("base_url"):
        embed_table.add_row("Base URL", embedding["base_url"])
    console.print(embed_table)

    # Summarization provider
    summarization = config.get("summarization", {})
    summ_table = Table(title="Summarization Provider", show_header=False)
    summ_table.add_column("Setting", style="cyan")
    summ_table.add_column("Value")
    summ_table.add_row("Provider", summarization.get("provider", "anthropic"))
    summ_table.add_row("Model", summarization.get("model", "claude-haiku-4-5-20251001"))
    summ_table.add_row(
        "API Key Env", summarization.get("api_key_env", "ANTHROPIC_API_KEY")
    )
    if summarization.get("base_url"):
        summ_table.add_row("Base URL", summarization["base_url"])
    console.print(summ_table)

    # Reranker (if configured)
    reranker = config.get("reranker", {})
    if reranker:
        rerank_table = Table(title="Reranker Provider", show_header=False)
        rerank_table.add_column("Setting", style="cyan")
        rerank_table.add_column("Value")
        rerank_table.add_row(
            "Provider", reranker.get("provider", "sentence-transformers")
        )
        rerank_table.add_row(
            "Model", reranker.get("model", "cross-encoder/ms-marco-MiniLM-L-6-v2")
        )
        if reranker.get("base_url"):
            rerank_table.add_row("Base URL", reranker["base_url"])
        console.print(rerank_table)

    console.print()


@config_group.command("path")
@click.option("--json", "json_output", is_flag=True, help="Output as JSON")
def config_path(json_output: bool) -> None:
    """Show the path to the active config file.

    \b
    Examples:
      agent-brain config path           # Print config file path
      agent-brain config path --json    # JSON output
    """
    config_path = _find_config_file()

    if json_output:
        click.echo(
            json.dumps({
                "config_file": str(config_path) if config_path else None,
                "exists": config_path.exists() if config_path else False,
            })
        )
        return

    if config_path:
        console.print(f"[green]{config_path}[/]")
    else:
        console.print("[yellow]No config file found[/]")
```
  </action>
  <verify>Run `cd agent-brain-cli && poetry run python -c "from agent_brain_cli.commands.config import config_group; print('Config command imported successfully')"` to verify import works.</verify>
  <done>Config CLI command module created with show and path subcommands.</done>
</task>

<task type="auto">
  <name>Task 4: Register config command in CLI main module</name>
  <files>agent-brain-cli/agent_brain_cli/cli.py</files>
  <action>
1. Add import for the config command group:

```python
from agent_brain_cli.commands.config import config_group
```

2. Register the command group with the main CLI:

```python
# In the CLI setup section, after other command registrations:
cli.add_command(config_group, name="config")
```

This makes `agent-brain config show` and `agent-brain config path` available.
  </action>
  <verify>Run `cd agent-brain-cli && poetry run agent-brain config --help` to verify the command group is registered.</verify>
  <done>Config command group registered in main CLI.</done>
</task>

<task type="auto">
  <name>Task 5: Add PyYAML dependency to CLI if not present</name>
  <files>agent-brain-cli/pyproject.toml</files>
  <action>
Check if PyYAML is already a dependency. If not, add it:

```bash
cd agent-brain-cli && poetry add pyyaml
```

Or manually add to pyproject.toml under [tool.poetry.dependencies]:

```toml
pyyaml = "^6.0"
```
  </action>
  <verify>Run `cd agent-brain-cli && poetry show pyyaml` to verify PyYAML is installed.</verify>
  <done>PyYAML dependency available in CLI package.</done>
</task>

</tasks>

<verification>
1. Run `task before-push` from the agent-brain directory to ensure all quality checks pass
2. Test config commands:
   - `agent-brain config show` - should display active configuration
   - `agent-brain config path` - should show config file path
   - `agent-brain config show --json` - should output JSON
3. Run E2E tests:
   - `cd agent-brain-server && poetry run pytest e2e/integration/test_provider_switching.py -v`
4. Verify provider switching works:
   - Create config with OpenAI, note embedding dimensions
   - Change to Ollama config
   - Verify config show reflects the change
5. Run full test suite: `poetry run pytest`
</verification>

<success_criteria>
- E2E test file exists with tests for provider switching
- Test fixtures exist for OpenAI, Ollama, and Cohere configurations
- agent-brain config show displays active configuration
- agent-brain config path shows config file location
- Tests verify dimension mismatch detection works
- All tests pass
- `task before-push` passes
</success_criteria>

<output>
After completion, create `.planning/phases/02-pluggable-providers/02-03-SUMMARY.md`
</output>
