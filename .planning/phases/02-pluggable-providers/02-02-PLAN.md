---
phase: 02-pluggable-providers
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - agent-brain-server/agent_brain_server/config/provider_config.py
  - agent-brain-server/agent_brain_server/api/main.py
  - agent-brain-server/agent_brain_server/api/routers/health.py
  - agent-brain-server/agent_brain_server/models/health.py
  - agent-brain-cli/agent_brain_cli/commands/start.py
  - agent-brain-server/tests/unit/config/test_provider_validation.py
autonomous: true

must_haves:
  truths:
    - "Validation errors have severity levels (CRITICAL, WARNING)"
    - "CRITICAL errors cause startup failure when --strict mode is enabled"
    - "WARNING errors are logged but don't block startup"
    - "/health/providers endpoint shows provider status with health checks"
    - "CLI start command supports --strict flag"
  artifacts:
    - path: "agent-brain-server/agent_brain_server/config/provider_config.py"
      provides: "ValidationError class with severity"
      contains: "class ValidationError"
    - path: "agent-brain-server/agent_brain_server/api/routers/health.py"
      provides: "/providers endpoint"
      contains: "def providers_status"
    - path: "agent-brain-cli/agent_brain_cli/commands/start.py"
      provides: "--strict flag"
      contains: "--strict"
  key_links:
    - from: "api/main.py"
      to: "config/provider_config.py"
      via: "ValidationError import"
      pattern: "from.*provider_config.*import.*ValidationError"
---

<objective>
Implement strict startup validation with severity levels and provider health endpoint.

Purpose: Allow users to catch configuration errors early with fail-fast behavior (--strict mode) while maintaining backward compatibility with current warning-only behavior.

Output: Validation severity levels, --strict CLI flag, and /health/providers endpoint for debugging.
</objective>

<execution_context>
@/Users/richardhightower/.claude/get-shit-done/workflows/execute-plan.md
@/Users/richardhightower/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/02-pluggable-providers/02-RESEARCH.md
@agent-brain-server/agent_brain_server/config/provider_config.py
@agent-brain-server/agent_brain_server/api/main.py
@agent-brain-server/agent_brain_server/api/routers/health.py
@agent-brain-cli/agent_brain_cli/commands/start.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add ValidationError class with severity levels</name>
  <files>agent-brain-server/agent_brain_server/config/provider_config.py</files>
  <action>
1. Add enum and dataclass imports at the top:

```python
from dataclasses import dataclass
from enum import Enum
```

2. Add severity enum and ValidationError class after the imports, before EmbeddingConfig:

```python
class ValidationSeverity(str, Enum):
    """Severity level for validation errors."""

    CRITICAL = "critical"  # Blocks startup in strict mode
    WARNING = "warning"    # Logged but doesn't block startup


@dataclass
class ValidationError:
    """A validation error with severity and details."""

    message: str
    severity: ValidationSeverity
    provider_type: str  # "embedding", "summarization", "reranker"
    field: str = ""     # Optional field name

    def __str__(self) -> str:
        prefix = "[CRITICAL]" if self.severity == ValidationSeverity.CRITICAL else "[WARNING]"
        return f"{prefix} {self.provider_type}: {self.message}"
```

3. Update `validate_provider_config` to return ValidationError objects instead of strings:

```python
def validate_provider_config(settings: ProviderSettings) -> list[ValidationError]:
    """Validate provider configuration and return list of errors.

    Checks:
    - API keys are available for providers that need them (CRITICAL)
    - Provider health checks pass (WARNING)

    Args:
        settings: Provider settings to validate

    Returns:
        List of ValidationError objects (empty if valid)
    """
    errors: list[ValidationError] = []

    # Validate embedding provider
    if settings.embedding.provider != EmbeddingProviderType.OLLAMA:
        api_key = settings.embedding.get_api_key()
        if not api_key:
            env_var = settings.embedding.api_key_env or "OPENAI_API_KEY"
            errors.append(ValidationError(
                message=(
                    f"Missing API key for {settings.embedding.provider} embeddings. "
                    f"Set {env_var} environment variable."
                ),
                severity=ValidationSeverity.CRITICAL,
                provider_type="embedding",
                field="api_key",
            ))

    # Validate summarization provider
    if settings.summarization.provider != SummarizationProviderType.OLLAMA:
        api_key = settings.summarization.get_api_key()
        if not api_key:
            env_var = settings.summarization.api_key_env or "ANTHROPIC_API_KEY"
            errors.append(ValidationError(
                message=(
                    f"Missing API key for {settings.summarization.provider} summarization. "
                    f"Set {env_var} environment variable."
                ),
                severity=ValidationSeverity.CRITICAL,
                provider_type="summarization",
                field="api_key",
            ))

    return errors


def has_critical_errors(errors: list[ValidationError]) -> bool:
    """Check if any validation errors are critical.

    Args:
        errors: List of validation errors

    Returns:
        True if any error has CRITICAL severity
    """
    return any(e.severity == ValidationSeverity.CRITICAL for e in errors)
```

4. Add export for new classes in the module's imports if using __all__:

```python
# If there's an __all__ list, add:
# "ValidationSeverity", "ValidationError", "has_critical_errors"
```
  </action>
  <verify>Run `cd agent-brain-server && poetry run python -c "from agent_brain_server.config.provider_config import ValidationError, ValidationSeverity, has_critical_errors; e = ValidationError('test', ValidationSeverity.CRITICAL, 'embedding'); print(str(e))"` to verify the classes work.</verify>
  <done>ValidationError class with severity levels exists and validate_provider_config returns structured errors.</done>
</task>

<task type="auto">
  <name>Task 2: Add provider health endpoint to health router</name>
  <files>agent-brain-server/agent_brain_server/api/routers/health.py, agent-brain-server/agent_brain_server/models/health.py</files>
  <action>
1. First, add the response model to `agent-brain-server/agent_brain_server/models/health.py`:

```python
from pydantic import BaseModel, Field
from typing import Optional
from datetime import datetime


class ProviderHealth(BaseModel):
    """Health status for a single provider."""

    provider_type: str = Field(description="Type: embedding, summarization, reranker")
    provider_name: str = Field(description="Provider name (e.g., openai, ollama)")
    model: str = Field(description="Model being used")
    status: str = Field(description="Status: healthy, degraded, unavailable")
    message: Optional[str] = Field(default=None, description="Status message")
    dimensions: Optional[int] = Field(
        default=None, description="Embedding dimensions (for embedding providers)"
    )


class ProvidersStatus(BaseModel):
    """Status of all configured providers."""

    config_source: Optional[str] = Field(
        default=None, description="Path to config file if loaded"
    )
    strict_mode: bool = Field(
        default=False, description="Whether strict validation is enabled"
    )
    validation_errors: list[str] = Field(
        default_factory=list, description="Validation error messages"
    )
    providers: list[ProviderHealth] = Field(
        default_factory=list, description="Status of each provider"
    )
    timestamp: datetime = Field(description="Status check timestamp")
```

2. Update `agent-brain-server/agent_brain_server/api/routers/health.py`:

Add imports:

```python
from agent_brain_server.models.health import ProviderHealth, ProvidersStatus
from agent_brain_server.config.provider_config import (
    load_provider_settings,
    validate_provider_config,
    _find_config_file,
)
from agent_brain_server.providers.factory import ProviderRegistry
```

Add the new endpoint:

```python
@router.get(
    "/providers",
    response_model=ProvidersStatus,
    summary="Provider Status",
    description="Returns status of all configured providers with health checks.",
)
async def providers_status(request: Request) -> ProvidersStatus:
    """Get detailed status of all configured providers.

    Returns:
        ProvidersStatus with configuration source, validation errors,
        and health status of each provider.
    """
    # Get config source
    config_file = _find_config_file()
    config_source = str(config_file) if config_file else None

    # Get strict mode from app state
    strict_mode = getattr(request.app.state, "strict_mode", False)

    # Load settings and validate
    settings = load_provider_settings()
    validation_errors = validate_provider_config(settings)
    error_messages = [str(e) for e in validation_errors]

    providers: list[ProviderHealth] = []

    # Check embedding provider
    try:
        embedding_provider = ProviderRegistry.get_embedding_provider(settings.embedding)
        embedding_status = "healthy"
        embedding_message = None
        embedding_dimensions = embedding_provider.get_dimensions()
    except Exception as e:
        embedding_status = "unavailable"
        embedding_message = str(e)
        embedding_dimensions = None

    providers.append(ProviderHealth(
        provider_type="embedding",
        provider_name=str(settings.embedding.provider),
        model=settings.embedding.model,
        status=embedding_status,
        message=embedding_message,
        dimensions=embedding_dimensions,
    ))

    # Check summarization provider
    try:
        summarization_provider = ProviderRegistry.get_summarization_provider(
            settings.summarization
        )
        summarization_status = "healthy"
        summarization_message = None
    except Exception as e:
        summarization_status = "unavailable"
        summarization_message = str(e)

    providers.append(ProviderHealth(
        provider_type="summarization",
        provider_name=str(settings.summarization.provider),
        model=settings.summarization.model,
        status=summarization_status,
        message=summarization_message,
    ))

    # Check reranker provider if reranking is enabled
    from agent_brain_server.config import settings as app_settings
    if app_settings.ENABLE_RERANKING:
        try:
            reranker_provider = ProviderRegistry.get_reranker_provider(
                settings.reranker
            )
            reranker_status = "healthy"
            reranker_message = None
        except Exception as e:
            reranker_status = "unavailable"
            reranker_message = str(e)

        providers.append(ProviderHealth(
            provider_type="reranker",
            provider_name=str(settings.reranker.provider),
            model=settings.reranker.model,
            status=reranker_status,
            message=reranker_message,
        ))

    return ProvidersStatus(
        config_source=config_source,
        strict_mode=strict_mode,
        validation_errors=error_messages,
        providers=providers,
        timestamp=datetime.now(timezone.utc),
    )
```
  </action>
  <verify>Run `cd agent-brain-server && poetry run python -c "from agent_brain_server.api.routers.health import providers_status; print('Endpoint imported successfully')"` to verify the endpoint is importable.</verify>
  <done>/health/providers endpoint exists and returns provider status with health checks.</done>
</task>

<task type="auto">
  <name>Task 3: Add strict mode to FastAPI lifespan</name>
  <files>agent-brain-server/agent_brain_server/api/main.py</files>
  <action>
1. Add imports:

```python
from agent_brain_server.config.provider_config import (
    clear_settings_cache,
    load_provider_settings,
    validate_provider_config,
    has_critical_errors,
    ValidationSeverity,
)
```

2. Add strict mode environment variable to settings.py (if not exists):

In `agent-brain-server/agent_brain_server/config/settings.py`, add:

```python
# Strict Mode Configuration
AGENT_BRAIN_STRICT_MODE: bool = False  # Fail on critical validation errors
```

3. Update the validation section in lifespan():

Replace the existing validation block:

```python
# Load and validate provider configuration
# Clear cache first to ensure we pick up env vars set by CLI
clear_settings_cache()
try:
    provider_settings = load_provider_settings()
    validation_errors = validate_provider_config(provider_settings)

    if validation_errors:
        for error in validation_errors:
            logger.warning(f"Provider config warning: {error}")
        # Log but don't fail - providers may work if keys are set later
        # or if using Ollama which doesn't need keys
```

With:

```python
# Load and validate provider configuration
# Clear cache first to ensure we pick up env vars set by CLI
clear_settings_cache()
strict_mode = settings.AGENT_BRAIN_STRICT_MODE

try:
    provider_settings = load_provider_settings()
    validation_errors = validate_provider_config(provider_settings)

    if validation_errors:
        for error in validation_errors:
            if error.severity == ValidationSeverity.CRITICAL:
                logger.error(f"Provider config error: {error}")
            else:
                logger.warning(f"Provider config warning: {error}")

        # In strict mode, fail on critical errors
        if strict_mode and has_critical_errors(validation_errors):
            critical_msgs = [
                str(e) for e in validation_errors
                if e.severity == ValidationSeverity.CRITICAL
            ]
            raise RuntimeError(
                f"Critical provider configuration errors (strict mode): "
                f"{'; '.join(critical_msgs)}"
            )
```

4. Store strict mode on app.state for the health endpoint:

After the provider validation section, add:

```python
app.state.strict_mode = strict_mode
```
  </action>
  <verify>Run `cd agent-brain-server && AGENT_BRAIN_STRICT_MODE=true poetry run python -c "from agent_brain_server.config import settings; print(f'Strict mode: {settings.AGENT_BRAIN_STRICT_MODE}')"` to verify the setting is read.</verify>
  <done>FastAPI lifespan fails on critical errors when strict mode is enabled.</done>
</task>

<task type="auto">
  <name>Task 4: Add --strict flag to CLI start command</name>
  <files>agent-brain-cli/agent_brain_cli/commands/start.py</files>
  <action>
1. Add the --strict option to the start command decorator:

```python
@click.option(
    "--strict",
    is_flag=True,
    help="Enable strict mode: fail on critical provider configuration errors",
)
```

2. Update the function signature:

```python
def start_command(
    path: Optional[str],
    host: Optional[str],
    port: Optional[int],
    foreground: bool,
    timeout: int,
    json_output: bool,
    strict: bool,
) -> None:
```

3. Add strict mode to environment variables before server launch:

In the section where environment variables are set (before `server_cmd`), add:

```python
# Set environment variables for server
env = os.environ.copy()
env["AGENT_BRAIN_PROJECT_ROOT"] = str(project_root)
env["AGENT_BRAIN_STATE_DIR"] = str(state_dir)
if strict:
    env["AGENT_BRAIN_STRICT_MODE"] = "true"
```

4. Update the CLI help text to mention strict mode:

```python
"""Start an Agent Brain server for this project.

Spawns a new server instance bound to the project. If a server is
already running for this project, reports its URL instead.

\b
Examples:
  agent-brain start                    # Start server for current project
  agent-brain start --port 8080        # Start on specific port
  agent-brain start --strict           # Fail on missing API keys
  agent-brain start --foreground       # Run in foreground
  agent-brain start --path /my/project # Start for specific project
"""
```
  </action>
  <verify>Run `cd agent-brain-cli && poetry run agent-brain start --help` to verify --strict flag appears in help output.</verify>
  <done>CLI start command has --strict flag that sets AGENT_BRAIN_STRICT_MODE=true.</done>
</task>

<task type="auto">
  <name>Task 5: Add unit tests for validation severity</name>
  <files>agent-brain-server/tests/unit/config/test_provider_validation.py</files>
  <action>
Create a new test file:

```python
"""Tests for provider configuration validation with severity levels."""

import os
import pytest
from unittest.mock import patch

from agent_brain_server.config.provider_config import (
    EmbeddingConfig,
    SummarizationConfig,
    ProviderSettings,
    ValidationError,
    ValidationSeverity,
    validate_provider_config,
    has_critical_errors,
)
from agent_brain_server.providers.base import (
    EmbeddingProviderType,
    SummarizationProviderType,
)


class TestValidationError:
    """Tests for ValidationError class."""

    def test_critical_error_string(self) -> None:
        """Test CRITICAL error string representation."""
        error = ValidationError(
            message="Missing API key",
            severity=ValidationSeverity.CRITICAL,
            provider_type="embedding",
            field="api_key",
        )
        result = str(error)
        assert "[CRITICAL]" in result
        assert "embedding" in result
        assert "Missing API key" in result

    def test_warning_error_string(self) -> None:
        """Test WARNING error string representation."""
        error = ValidationError(
            message="Provider may be unavailable",
            severity=ValidationSeverity.WARNING,
            provider_type="summarization",
        )
        result = str(error)
        assert "[WARNING]" in result
        assert "summarization" in result


class TestValidateProviderConfig:
    """Tests for validate_provider_config function."""

    def test_valid_config_with_env_keys(self) -> None:
        """Test validation passes when env vars are set."""
        with patch.dict(os.environ, {
            "OPENAI_API_KEY": "sk-test-key",
            "ANTHROPIC_API_KEY": "sk-ant-test",
        }):
            settings = ProviderSettings()
            errors = validate_provider_config(settings)
            assert len(errors) == 0

    def test_missing_embedding_key_is_critical(self) -> None:
        """Test missing embedding API key is CRITICAL severity."""
        with patch.dict(os.environ, {
            "ANTHROPIC_API_KEY": "sk-ant-test",
        }, clear=True):
            # Clear OPENAI_API_KEY
            os.environ.pop("OPENAI_API_KEY", None)

            settings = ProviderSettings(
                embedding=EmbeddingConfig(
                    provider=EmbeddingProviderType.OPENAI,
                    api_key=None,
                    api_key_env="OPENAI_API_KEY",
                ),
            )
            errors = validate_provider_config(settings)

            embedding_errors = [e for e in errors if e.provider_type == "embedding"]
            assert len(embedding_errors) == 1
            assert embedding_errors[0].severity == ValidationSeverity.CRITICAL

    def test_ollama_no_key_required(self) -> None:
        """Test Ollama provider doesn't require API key."""
        settings = ProviderSettings(
            embedding=EmbeddingConfig(
                provider=EmbeddingProviderType.OLLAMA,
                model="nomic-embed-text",
            ),
            summarization=SummarizationConfig(
                provider=SummarizationProviderType.OLLAMA,
                model="llama3.2",
            ),
        )
        errors = validate_provider_config(settings)
        # Should have no CRITICAL errors for missing keys
        critical = [e for e in errors if e.severity == ValidationSeverity.CRITICAL]
        assert len(critical) == 0


class TestHasCriticalErrors:
    """Tests for has_critical_errors function."""

    def test_returns_true_with_critical(self) -> None:
        """Test returns True when critical error present."""
        errors = [
            ValidationError("warn", ValidationSeverity.WARNING, "test"),
            ValidationError("crit", ValidationSeverity.CRITICAL, "test"),
        ]
        assert has_critical_errors(errors) is True

    def test_returns_false_with_only_warnings(self) -> None:
        """Test returns False when only warnings present."""
        errors = [
            ValidationError("warn1", ValidationSeverity.WARNING, "test"),
            ValidationError("warn2", ValidationSeverity.WARNING, "test"),
        ]
        assert has_critical_errors(errors) is False

    def test_returns_false_with_empty_list(self) -> None:
        """Test returns False with empty list."""
        assert has_critical_errors([]) is False
```
  </action>
  <verify>Run `cd agent-brain-server && poetry run pytest tests/unit/config/test_provider_validation.py -v` to run the tests.</verify>
  <done>Unit tests cover validation severity levels and has_critical_errors function.</done>
</task>

</tasks>

<verification>
1. Run `task before-push` from the agent-brain directory to ensure all quality checks pass
2. Test strict mode behavior:
   - Start server without API keys: `agent-brain start` (should warn but start)
   - Start with strict mode: `agent-brain start --strict` (should fail without API keys)
3. Test /health/providers endpoint:
   - `curl http://localhost:8000/health/providers` should return provider status
4. Verify validation error messages include severity level
5. Run full test suite: `poetry run pytest`
</verification>

<success_criteria>
- ValidationError class exists with severity (CRITICAL/WARNING)
- validate_provider_config returns ValidationError objects
- has_critical_errors function works correctly
- FastAPI lifespan fails on critical errors in strict mode
- CLI start has --strict flag that enables strict mode
- /health/providers endpoint returns provider status
- Unit tests pass with >80% coverage for new code
- `task before-push` passes
</success_criteria>

<output>
After completion, create `.planning/phases/02-pluggable-providers/02-02-SUMMARY.md`
</output>
