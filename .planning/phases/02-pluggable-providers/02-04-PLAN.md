---
phase: 02-pluggable-providers
plan: 04
type: execute
wave: 2
depends_on: ["02-02"]
files_modified:
  - e2e/integration/test_ollama_offline.py
  - e2e/fixtures/config_ollama_only.yaml
autonomous: true

must_haves:
  truths:
    - "E2E test runs with Ollama-only configuration"
    - "Test verifies no external API calls are made"
    - "Test gracefully handles Ollama not running"
    - "Test documents fully offline operation capability"
  artifacts:
    - path: "e2e/integration/test_ollama_offline.py"
      provides: "E2E test for Ollama offline mode"
      contains: "test_ollama_offline"
    - path: "e2e/fixtures/config_ollama_only.yaml"
      provides: "Ollama-only configuration"
      contains: "provider: ollama"
  key_links: []
---

<objective>
Create E2E test verifying fully offline operation with Ollama-only configuration.

Purpose: Prove that Agent Brain can operate without any external API calls when using Ollama for both embeddings and summarization (PROV-04 verification).

Output: E2E test for Ollama offline mode with proper graceful degradation when Ollama is unavailable.
</objective>

<execution_context>
@/Users/richardhightower/.claude/get-shit-done/workflows/execute-plan.md
@/Users/richardhightower/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/02-pluggable-providers/02-RESEARCH.md
@.planning/phases/02-pluggable-providers/02-02-PLAN.md
@agent-brain-server/agent_brain_server/providers/embedding/ollama.py
@agent-brain-server/agent_brain_server/providers/summarization/ollama.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Ollama-only configuration fixture</name>
  <files>e2e/fixtures/config_ollama_only.yaml</files>
  <action>
Create the configuration file:

```yaml
# Ollama-only configuration for fully offline operation
# No external API calls are made with this configuration

embedding:
  provider: ollama
  model: nomic-embed-text
  base_url: http://localhost:11434/v1
  params:
    # Ollama embedding batch size (reduce for memory-constrained systems)
    batch_size: 64

summarization:
  provider: ollama
  model: llama3.2
  base_url: http://localhost:11434/v1
  params:
    # Summarization parameters
    max_tokens: 500
    temperature: 0.3

# Reranker can also use Ollama for fully offline operation
reranker:
  provider: ollama
  model: llama3.2
  base_url: http://localhost:11434
```

This configuration ensures:
- No OpenAI API calls (embedding uses Ollama)
- No Anthropic API calls (summarization uses Ollama)
- Fully local operation
  </action>
  <verify>Run `cat e2e/fixtures/config_ollama_only.yaml` to verify the file exists with correct content.</verify>
  <done>Ollama-only configuration fixture created.</done>
</task>

<task type="auto">
  <name>Task 2: Create E2E test for Ollama offline mode</name>
  <files>e2e/integration/test_ollama_offline.py</files>
  <action>
Create the test file:

```python
"""E2E tests for Ollama offline mode (PROV-04 verification).

These tests verify that Agent Brain can operate fully offline using
Ollama for both embeddings and summarization, without making any
external API calls.
"""

import os
import shutil
import socket
import tempfile
from pathlib import Path
from typing import Any, Generator
from unittest.mock import AsyncMock, MagicMock, patch

import pytest

from agent_brain_server.config.provider_config import (
    clear_settings_cache,
    load_provider_settings,
    validate_provider_config,
)
from agent_brain_server.providers.base import (
    EmbeddingProviderType,
    SummarizationProviderType,
)
from agent_brain_server.providers.exceptions import OllamaConnectionError


# Path to fixture files
FIXTURES_DIR = Path(__file__).parent.parent / "fixtures"


def is_ollama_running(host: str = "localhost", port: int = 11434) -> bool:
    """Check if Ollama is running on the specified host:port."""
    try:
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            s.settimeout(1.0)
            s.connect((host, port))
            return True
    except (socket.error, socket.timeout):
        return False


@pytest.fixture
def temp_project_dir() -> Generator[Path, None, None]:
    """Create a temporary project directory with .claude/agent-brain structure."""
    with tempfile.TemporaryDirectory() as tmpdir:
        project_dir = Path(tmpdir)
        config_dir = project_dir / ".claude" / "agent-brain"
        config_dir.mkdir(parents=True)
        yield project_dir


@pytest.fixture(autouse=True)
def clear_config_cache() -> Generator[None, None, None]:
    """Clear the provider settings cache before and after each test."""
    clear_settings_cache()
    yield
    clear_settings_cache()


class TestOllamaConfiguration:
    """Tests for Ollama-only configuration."""

    def test_ollama_config_loads_correctly(self, temp_project_dir: Path) -> None:
        """Test Ollama-only config loads with correct provider types."""
        config_path = temp_project_dir / ".claude" / "agent-brain" / "config.yaml"
        shutil.copy(FIXTURES_DIR / "config_ollama_only.yaml", config_path)

        original_cwd = os.getcwd()
        try:
            os.chdir(temp_project_dir)
            clear_settings_cache()

            settings = load_provider_settings()

            # Verify embedding provider is Ollama
            assert settings.embedding.provider == EmbeddingProviderType.OLLAMA
            assert settings.embedding.model == "nomic-embed-text"
            assert "localhost:11434" in (settings.embedding.get_base_url() or "")

            # Verify summarization provider is Ollama
            assert settings.summarization.provider == SummarizationProviderType.OLLAMA
            assert settings.summarization.model == "llama3.2"

        finally:
            os.chdir(original_cwd)

    def test_ollama_no_api_keys_required(self, temp_project_dir: Path) -> None:
        """Test Ollama config doesn't require any API keys."""
        config_path = temp_project_dir / ".claude" / "agent-brain" / "config.yaml"
        shutil.copy(FIXTURES_DIR / "config_ollama_only.yaml", config_path)

        original_cwd = os.getcwd()
        try:
            os.chdir(temp_project_dir)
            clear_settings_cache()

            settings = load_provider_settings()

            # Verify no API keys are needed
            assert settings.embedding.get_api_key() is None
            assert settings.summarization.get_api_key() is None

            # Verify validation passes (no critical errors)
            errors = validate_provider_config(settings)
            critical = [e for e in errors if e.severity.value == "critical"]
            assert len(critical) == 0, f"Unexpected critical errors: {critical}"

        finally:
            os.chdir(original_cwd)


class TestNoExternalApiCalls:
    """Tests verifying no external API calls are made with Ollama config."""

    def test_no_openai_calls(self, temp_project_dir: Path) -> None:
        """Test that no OpenAI API calls are made."""
        config_path = temp_project_dir / ".claude" / "agent-brain" / "config.yaml"
        shutil.copy(FIXTURES_DIR / "config_ollama_only.yaml", config_path)

        original_cwd = os.getcwd()
        try:
            os.chdir(temp_project_dir)
            clear_settings_cache()

            # Mock httpx to track outgoing requests
            with patch("httpx.AsyncClient.post") as mock_post:
                with patch("httpx.AsyncClient.get") as mock_get:
                    settings = load_provider_settings()

                    # Create embedding provider (mocked Ollama)
                    with patch(
                        "agent_brain_server.providers.embedding.ollama.httpx"
                    ) as mock_httpx:
                        mock_client = MagicMock()
                        mock_httpx.AsyncClient.return_value.__aenter__.return_value = (
                            mock_client
                        )

                        from agent_brain_server.providers.factory import ProviderRegistry

                        provider = ProviderRegistry.get_embedding_provider(
                            settings.embedding
                        )

                        # Check that provider is Ollama
                        assert provider.provider_name == "ollama"

                        # Verify no calls to api.openai.com
                        for call in mock_post.call_args_list:
                            url = str(call[0][0]) if call[0] else ""
                            assert "openai.com" not in url
                            assert "anthropic.com" not in url

        finally:
            os.chdir(original_cwd)

    def test_no_anthropic_calls(self, temp_project_dir: Path) -> None:
        """Test that no Anthropic API calls are made."""
        config_path = temp_project_dir / ".claude" / "agent-brain" / "config.yaml"
        shutil.copy(FIXTURES_DIR / "config_ollama_only.yaml", config_path)

        original_cwd = os.getcwd()
        try:
            os.chdir(temp_project_dir)
            clear_settings_cache()

            settings = load_provider_settings()

            # Mock the anthropic client to ensure it's never called
            with patch("anthropic.AsyncAnthropic") as mock_anthropic:
                from agent_brain_server.providers.factory import ProviderRegistry

                provider = ProviderRegistry.get_summarization_provider(
                    settings.summarization
                )

                # Check that provider is Ollama
                assert provider.provider_name == "ollama"

                # Anthropic client should not be instantiated
                mock_anthropic.assert_not_called()

        finally:
            os.chdir(original_cwd)


class TestOllamaGracefulDegradation:
    """Tests for graceful degradation when Ollama is unavailable."""

    def test_connection_error_when_ollama_down(self, temp_project_dir: Path) -> None:
        """Test appropriate error when Ollama is not running."""
        config_path = temp_project_dir / ".claude" / "agent-brain" / "config.yaml"
        shutil.copy(FIXTURES_DIR / "config_ollama_only.yaml", config_path)

        original_cwd = os.getcwd()
        try:
            os.chdir(temp_project_dir)
            clear_settings_cache()

            settings = load_provider_settings()

            # Mock Ollama being unavailable
            with patch(
                "agent_brain_server.providers.embedding.ollama.httpx.AsyncClient"
            ) as mock_client:
                import httpx

                mock_instance = MagicMock()
                mock_client.return_value.__aenter__.return_value = mock_instance
                mock_instance.post.side_effect = httpx.ConnectError(
                    "Connection refused"
                )

                from agent_brain_server.providers.factory import ProviderRegistry

                provider = ProviderRegistry.get_embedding_provider(settings.embedding)

                # Attempt to embed should raise OllamaConnectionError
                with pytest.raises(OllamaConnectionError) as exc_info:
                    import asyncio

                    asyncio.get_event_loop().run_until_complete(
                        provider.embed_text("test")
                    )

                assert "Cannot connect to Ollama" in str(exc_info.value)
                assert "localhost:11434" in str(exc_info.value)

        finally:
            os.chdir(original_cwd)

    def test_health_check_reports_ollama_status(self, temp_project_dir: Path) -> None:
        """Test that health check accurately reports Ollama provider status."""
        config_path = temp_project_dir / ".claude" / "agent-brain" / "config.yaml"
        shutil.copy(FIXTURES_DIR / "config_ollama_only.yaml", config_path)

        original_cwd = os.getcwd()
        try:
            os.chdir(temp_project_dir)
            clear_settings_cache()

            settings = load_provider_settings()

            # Check if Ollama is actually running
            ollama_running = is_ollama_running()

            from agent_brain_server.providers.factory import ProviderRegistry

            try:
                provider = ProviderRegistry.get_embedding_provider(settings.embedding)
                provider_created = True
            except Exception:
                provider_created = False

            # Provider should be created regardless of Ollama status
            # (actual connection happens on first embed call)
            if provider_created:
                assert provider.provider_name == "ollama"

        finally:
            os.chdir(original_cwd)


@pytest.mark.skipif(
    not is_ollama_running(),
    reason="Ollama not running - skipping live integration test",
)
class TestOllamaLiveIntegration:
    """Live integration tests that require Ollama to be running.

    These tests are skipped if Ollama is not available.
    Run with: pytest -m "not skip" to include when Ollama is running.
    """

    def test_ollama_embedding_returns_vector(self, temp_project_dir: Path) -> None:
        """Test that Ollama returns actual embeddings when running."""
        config_path = temp_project_dir / ".claude" / "agent-brain" / "config.yaml"
        shutil.copy(FIXTURES_DIR / "config_ollama_only.yaml", config_path)

        original_cwd = os.getcwd()
        try:
            os.chdir(temp_project_dir)
            clear_settings_cache()

            settings = load_provider_settings()

            from agent_brain_server.providers.factory import ProviderRegistry

            provider = ProviderRegistry.get_embedding_provider(settings.embedding)

            # Actually call Ollama
            import asyncio

            embedding = asyncio.get_event_loop().run_until_complete(
                provider.embed_text("Hello, world!")
            )

            # Verify we got a vector
            assert isinstance(embedding, list)
            assert len(embedding) > 0
            assert all(isinstance(x, float) for x in embedding)

            # nomic-embed-text produces 768-dimensional vectors
            assert len(embedding) == 768

        finally:
            os.chdir(original_cwd)

    def test_ollama_summarization_returns_text(self, temp_project_dir: Path) -> None:
        """Test that Ollama returns actual summaries when running."""
        config_path = temp_project_dir / ".claude" / "agent-brain" / "config.yaml"
        shutil.copy(FIXTURES_DIR / "config_ollama_only.yaml", config_path)

        original_cwd = os.getcwd()
        try:
            os.chdir(temp_project_dir)
            clear_settings_cache()

            settings = load_provider_settings()

            from agent_brain_server.providers.factory import ProviderRegistry

            provider = ProviderRegistry.get_summarization_provider(
                settings.summarization
            )

            # Actually call Ollama
            import asyncio

            code = '''
            def hello():
                """Say hello to the world."""
                print("Hello, world!")
            '''

            summary = asyncio.get_event_loop().run_until_complete(
                provider.summarize(code)
            )

            # Verify we got text back
            assert isinstance(summary, str)
            assert len(summary) > 10

        finally:
            os.chdir(original_cwd)


class TestOfflineDocumentation:
    """Tests that document offline operation capabilities."""

    def test_offline_config_documented(self) -> None:
        """Verify offline configuration fixture is properly documented."""
        config_path = FIXTURES_DIR / "config_ollama_only.yaml"
        content = config_path.read_text()

        # Verify documentation is present
        assert "fully offline" in content.lower() or "offline" in content.lower()
        assert "no external api" in content.lower() or "no api calls" in content.lower()

    def test_all_providers_support_ollama(self) -> None:
        """Verify all provider types support Ollama backend."""
        from agent_brain_server.providers.base import (
            EmbeddingProviderType,
            SummarizationProviderType,
            RerankerProviderType,
        )

        # All provider types should have OLLAMA option
        assert EmbeddingProviderType.OLLAMA.value == "ollama"
        assert SummarizationProviderType.OLLAMA.value == "ollama"
        assert RerankerProviderType.OLLAMA.value == "ollama"
```
  </action>
  <verify>Run `cd agent-brain-server && poetry run pytest e2e/integration/test_ollama_offline.py -v --tb=short` to run the tests.</verify>
  <done>E2E test file created for Ollama offline mode verification.</done>
</task>

<task type="auto">
  <name>Task 3: Add pytest marker for Ollama tests</name>
  <files>agent-brain-server/pytest.ini or agent-brain-server/pyproject.toml</files>
  <action>
Add pytest marker configuration for Ollama tests. In `pyproject.toml` under `[tool.pytest.ini_options]`:

```toml
[tool.pytest.ini_options]
markers = [
    "ollama: marks tests that require Ollama to be running",
    "slow: marks tests as slow (deselect with '-m \"not slow\"')",
]
```

Or in `pytest.ini`:

```ini
[pytest]
markers =
    ollama: marks tests that require Ollama to be running
    slow: marks tests as slow
```

This allows running:
- `pytest -m "not ollama"` to skip Ollama tests when Ollama is down
- `pytest -m ollama` to run only Ollama tests
  </action>
  <verify>Run `cd agent-brain-server && poetry run pytest --markers | grep ollama` to verify the marker is registered.</verify>
  <done>Pytest marker configured for Ollama-dependent tests.</done>
</task>

<task type="auto">
  <name>Task 4: Update PROV-04 documentation</name>
  <files>e2e/integration/test_ollama_offline.py</files>
  <action>
Add comprehensive docstring at the top of the test file documenting PROV-04:

```python
"""E2E tests for Ollama offline mode (PROV-04 verification).

PROV-04 Requirement: Fully offline operation with Ollama
=========================================================

Agent Brain must be able to operate fully offline when configured with
Ollama for both embeddings and summarization. This means:

1. No external API calls to OpenAI, Anthropic, Cohere, or other cloud services
2. No API keys required for operation
3. Graceful error handling when Ollama is unavailable
4. Clear documentation of offline configuration

Test Categories
---------------

1. Configuration Tests (TestOllamaConfiguration)
   - Verify Ollama-only config loads correctly
   - Verify no API keys are required

2. No External API Tests (TestNoExternalApiCalls)
   - Verify no OpenAI calls are made
   - Verify no Anthropic calls are made

3. Graceful Degradation Tests (TestOllamaGracefulDegradation)
   - Verify appropriate errors when Ollama is down
   - Verify health check reports Ollama status

4. Live Integration Tests (TestOllamaLiveIntegration)
   - Test actual embedding generation (requires Ollama running)
   - Test actual summarization (requires Ollama running)

Running Tests
-------------

With Ollama running:
    pytest e2e/integration/test_ollama_offline.py -v

Without Ollama (skips live tests):
    pytest e2e/integration/test_ollama_offline.py -v

Only live tests:
    pytest e2e/integration/test_ollama_offline.py -v -k "Live"

Configuration
-------------

Use e2e/fixtures/config_ollama_only.yaml for fully offline operation.

Requirements:
- Ollama installed: https://ollama.ai
- Models pulled: ollama pull nomic-embed-text && ollama pull llama3.2
- Ollama running: ollama serve
"""
```

This docstring is already included in the test file from Task 2, but ensure it's complete.
  </action>
  <verify>Run `head -50 e2e/integration/test_ollama_offline.py` to verify documentation is present.</verify>
  <done>PROV-04 documentation added to test file.</done>
</task>

</tasks>

<verification>
1. Run `task before-push` from the agent-brain directory to ensure all quality checks pass
2. Test with Ollama running:
   - Start Ollama: `ollama serve`
   - Run tests: `pytest e2e/integration/test_ollama_offline.py -v`
   - Verify live tests pass
3. Test without Ollama:
   - Stop Ollama
   - Run tests: `pytest e2e/integration/test_ollama_offline.py -v`
   - Verify live tests are skipped
   - Verify graceful degradation tests pass
4. Verify no external API calls:
   - Check mock assertions in test output
   - Verify no calls to openai.com or anthropic.com
5. Run full test suite: `poetry run pytest`
</verification>

<success_criteria>
- config_ollama_only.yaml fixture exists with full offline config
- E2E test file exists with comprehensive Ollama tests
- Tests verify no external API calls are made
- Tests handle Ollama unavailability gracefully
- Live integration tests are properly skipped when Ollama is down
- pytest marker configured for Ollama tests
- Documentation clearly explains PROV-04 requirements
- All tests pass
- `task before-push` passes
</success_criteria>

<output>
After completion, create `.planning/phases/02-pluggable-providers/02-04-SUMMARY.md`
</output>
